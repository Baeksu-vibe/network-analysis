import streamlit as st
import pandas as pd
import re
from itertools import combinations
from collections import Counter
import io
import zipfile
import base64
import tempfile
import os
import networkx as nx
import numpy as np
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
from datetime import datetime, date
import matplotlib.pyplot as plt
import matplotlib.font_manager as fm
from matplotlib import font_manager
import seaborn as sns

# í˜ì´ì§€ ì„¤ì •
st.set_page_config(
    page_title="íŠ¹í—ˆ ë„¤íŠ¸ì›Œí¬ ë¶„ì„ ë° ì‹œê°í™” ë„êµ¬",
    page_icon="ğŸ“Š",
    layout="wide"
)

# í•œê¸€ í°íŠ¸ ì„¤ì • (Matplotlibìš©)
@st.cache_resource
def setup_korean_font():
    """í•œê¸€ í°íŠ¸ ì„¤ì •"""
    try:
        # ì‹œìŠ¤í…œì—ì„œ ì‚¬ìš© ê°€ëŠ¥í•œ í•œê¸€ í°íŠ¸ ì°¾ê¸°
        available_fonts = [f.name for f in font_manager.fontManager.ttflist]
        korean_fonts = ['Malgun Gothic', 'NanumGothic', 'AppleGothic', 'Gulim', 'Dotum']
        
        for font in korean_fonts:
            if font in available_fonts:
                plt.rcParams['font.family'] = font
                plt.rcParams['axes.unicode_minus'] = False
                return font
        
        # ê¸°ë³¸ í°íŠ¸ë¡œ ì„¤ì •
        plt.rcParams['font.family'] = 'DejaVu Sans'
        plt.rcParams['axes.unicode_minus'] = False
        return 'DejaVu Sans'
    except:
        plt.rcParams['font.family'] = 'DejaVu Sans'
        plt.rcParams['axes.unicode_minus'] = False
        return 'DejaVu Sans'

# ì•± ì œëª© ë° ì„¤ëª…
st.title("íŠ¹í—ˆ ë„¤íŠ¸ì›Œí¬ ë¶„ì„ ë° ì‹œê°í™” ë„êµ¬")
st.markdown("""
ì´ ë„êµ¬ëŠ” íŠ¹í—ˆ ë°ì´í„°ì—ì„œ ë‹¤ìŒ í•­ëª©ë“¤ì˜ ì‹œê³„ì—´ ë™ì‹œì¶œí˜„ë¹ˆë„ë¥¼ ë¶„ì„í•˜ê³  **ë„¤íŠ¸ì›Œí¬ ê·¸ë˜í”„ë¥¼ ì‹œê°í™”**í•©ë‹ˆë‹¤:
- **IPC ì½”ë“œ** ë™ì‹œì¶œí˜„ë¹ˆë„ (ì‹œê³„ì—´ ë¶„ì„)
- **ë°œëª…ì** ë™ì‹œì¶œí˜„ë¹ˆë„ (ì‹œê³„ì—´ ë¶„ì„)
- **ì¶œì›ì¸** ë™ì‹œì¶œí˜„ë¹ˆë„ (ì‹œê³„ì—´ ë¶„ì„)

**ğŸ†• ìƒˆë¡œìš´ ë„¤íŠ¸ì›Œí¬ ì‹œê°í™” ê¸°ëŠ¥**:
- **ì¸í„°ë™í‹°ë¸Œ ë„¤íŠ¸ì›Œí¬ ê·¸ë˜í”„** (Plotly ê¸°ë°˜)
- **ì¤‘ì‹¬ì„± ì§€í‘œ ê¸°ë°˜ ë…¸ë“œ í¬ê¸°/ìƒ‰ìƒ ì¡°ì •**
- **í•„í„°ë§ ë° í™•ëŒ€/ì¶•ì†Œ ê¸°ëŠ¥**
- **ë…¸ë“œ/ì—£ì§€ ì •ë³´ í˜¸ë²„ í‘œì‹œ**
- **ë„¤íŠ¸ì›Œí¬ êµ¬ì¡° ë¹„êµ ì‹œê°í™”**
- **ì»¤ë®¤ë‹ˆí‹° íƒì§€ ë° ì‹œê°í™”**
- **ë„¤íŠ¸ì›Œí¬ í†µê³„ ëŒ€ì‹œë³´ë“œ**

**ì¤‘ì‹¬ì„± ì§€í‘œ í¬í•¨**:
- **EC (Eigenvector Centrality)**: ê³ ìœ ë²¡í„° ì¤‘ì‹¬ì„±
- **BC (Betweenness Centrality)**: ë§¤ê°œ ì¤‘ì‹¬ì„±
- **CC (Closeness Centrality)**: ê·¼ì ‘ ì¤‘ì‹¬ì„±

**ì‹œê³„ì—´ ë¶„ì„ ê¸°ëŠ¥**:
- ì¶œì›ì¼ ê¸°ì¤€ ì‹œê³„ì—´ ë¶„ì„
- ìµœëŒ€ 3ê°œ êµ¬ê°„ ë¹„êµ ë¶„ì„
- êµ¬ê°„ë³„ ë„¤íŠ¸ì›Œí¬ ë³€í™” ì¶”ì 
- **êµ¬ê°„ë³„ ì¤‘ì‹¬ì„± ì§€í‘œ ì¦ê° ì¶”ì´ ë¶„ì„**
- **ì‹ ê·œ/ì†Œë©¸ ë…¸ë“œ ì¶”ì **
- **ì—°ë„ë³„ ì¤‘ì‹¬ì„± ì§€í‘œ ì‹œê°í™”**

Gephiì™€ ê°™ì€ ë„¤íŠ¸ì›Œí¬ ì‹œê°í™” ë„êµ¬ì—ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ë…¸ë“œì™€ ì—£ì§€ íŒŒì¼ì„ ìƒì„±í•©ë‹ˆë‹¤.
""")

# ê¸°ë³¸ ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤
def parse_application_date(date_str):
    """
    ì¶œì›ì¼ ë¬¸ìì—´ì„ datetime ê°ì²´ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
    ë‹¤ì–‘í•œ ë‚ ì§œ í˜•ì‹ì„ ì§€ì›í•©ë‹ˆë‹¤.
    """
    if pd.isna(date_str) or date_str == '' or date_str is None:
        return None
    
    # ë¬¸ìì—´ë¡œ ë³€í™˜í•˜ê³  ê³µë°± ì œê±°
    date_str = str(date_str).strip()
    
    if date_str == '' or date_str.lower() == 'nan':
        return None
    
    # ë‹¤ì–‘í•œ ë‚ ì§œ í˜•ì‹ íŒ¨í„´
    date_formats = [
        '%Y-%m-%d',
        '%Y/%m/%d', 
        '%Y.%m.%d',
        '%Y-%m',
        '%Y/%m',
        '%Y.%m',
        '%Y',
        '%Y-%m-%d %H:%M:%S',
        '%Y/%m/%d %H:%M:%S',
        '%m/%d/%Y',
        '%d/%m/%Y',
        '%d.%m.%Y',
        '%m-%d-%Y',
        '%d-%m-%Y'
    ]
    
    for fmt in date_formats:
        try:
            parsed_date = datetime.strptime(date_str, fmt)
            # ë…„ë„ê°€ ë„ˆë¬´ ë¯¸ë˜ë‚˜ ê³¼ê±°ì¸ ê²½ìš° í•„í„°ë§
            if 1990 <= parsed_date.year <= 2030:
                return parsed_date
        except ValueError:
            continue
    
    # ìˆ«ìë§Œ ìˆëŠ” ê²½ìš° (ì˜ˆ: 20230101)
    if date_str.isdigit():
        if len(date_str) == 8:  # YYYYMMDD
            try:
                parsed_date = datetime.strptime(date_str, '%Y%m%d')
                if 1990 <= parsed_date.year <= 2030:
                    return parsed_date
            except ValueError:
                pass
        elif len(date_str) == 6:  # YYYYMM
            try:
                parsed_date = datetime.strptime(date_str, '%Y%m')
                if 1990 <= parsed_date.year <= 2030:
                    return parsed_date
            except ValueError:
                pass
        elif len(date_str) == 4:  # YYYY
            try:
                parsed_date = datetime.strptime(date_str, '%Y')
                if 1990 <= parsed_date.year <= 2030:
                    return parsed_date
            except ValueError:
                pass
    
    # Excel ë‚ ì§œ ìˆ«ì í˜•ì‹ ì²˜ë¦¬ ì‹œë„
    try:
        # Excelì—ì„œ ë‚ ì§œê°€ ìˆ«ìë¡œ ì €ì¥ëœ ê²½ìš°
        excel_date = pd.to_datetime(date_str, errors='coerce')
        if pd.notna(excel_date) and 1990 <= excel_date.year <= 2030:
            return excel_date
    except:
        pass
    
    return None

def filter_data_by_period(df, date_column, start_year, end_year):
    """
    ì§€ì •ëœ ê¸°ê°„ìœ¼ë¡œ ë°ì´í„°ë¥¼ í•„í„°ë§í•©ë‹ˆë‹¤.
    """
    if date_column not in df.columns:
        st.error(f"'{date_column}' ì»¬ëŸ¼ì´ ë°ì´í„°í”„ë ˆì„ì— ì—†ìŠµë‹ˆë‹¤.")
        return df
    
    # ë‚ ì§œ íŒŒì‹±
    df_filtered = df.copy()
    df_filtered['parsed_date'] = df_filtered[date_column].apply(parse_application_date)
    
    # ìœ íš¨í•œ ë‚ ì§œê°€ ìˆëŠ” í–‰ë§Œ í•„í„°ë§ (Noneì´ ì•„ë‹Œ ê°’ë“¤ë§Œ)
    df_filtered = df_filtered[df_filtered['parsed_date'].notna()]
    
    if len(df_filtered) == 0:
        st.warning(f"ìœ íš¨í•œ ë‚ ì§œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return pd.DataFrame(columns=df.columns)
    
    # datetime íƒ€ì…ìœ¼ë¡œ ë³€í™˜
    df_filtered['parsed_date'] = pd.to_datetime(df_filtered['parsed_date'])
    
    # ì—°ë„ ì¶”ì¶œ
    df_filtered['year'] = df_filtered['parsed_date'].dt.year
    
    # ê¸°ê°„ í•„í„°ë§
    df_filtered = df_filtered[(df_filtered['year'] >= start_year) & (df_filtered['year'] <= end_year)]
    
    # ì„ì‹œ ì»¬ëŸ¼ ì œê±°í•˜ê³  ì›ë³¸ ë°ì´í„° ë°˜í™˜
    return df_filtered.drop(['parsed_date'], axis=1)

def extract_shortened_ipc_codes(ipc_str, code_length=4):
    """
    IPC ì½”ë“œ ë¬¸ìì—´ì—ì„œ ê°œë³„ IPC ì½”ë“œë“¤ì„ ì¶”ì¶œí•˜ê³  ì§€ì •ëœ ê¸¸ì´ë¡œ ì¶•ì•½í•©ë‹ˆë‹¤.
    code_length: 4(ì„¹ì…˜+í´ë˜ìŠ¤) ë˜ëŠ” 8(ì„œë¸Œí´ë˜ìŠ¤+ë©”ì¸ê·¸ë£¹)
    """
    if pd.isna(ipc_str) or ipc_str == '':
        return []
    
    # ë¬¸ìì—´ë¡œ ë³€í™˜ (ìˆ«ì ë“± ë‹¤ë¥¸ íƒ€ì…ì´ ë“¤ì–´ì˜¬ ê²½ìš° ëŒ€ë¹„)
    ipc_str = str(ipc_str)
    
    # ì›ë³¸ IPC ì½”ë“œ ì¶”ì¶œ
    codes = []
    if '[' in ipc_str and ']' in ipc_str:
        # ëŒ€ê´„í˜¸ ì•ˆê³¼ ë°–ì˜ ì½”ë“œë¥¼ ëª¨ë‘ ì¶”ì¶œ
        outside_brackets = ipc_str.split('[')[0].strip()
        inside_brackets = ipc_str.split('[')[1].split(']')[0].strip()
        
        if outside_brackets:
            codes.append(outside_brackets)
        
        if inside_brackets:
            inside_codes = [code.strip() for code in inside_brackets.split(',')]
            codes.extend(inside_codes)
    else:
        # ëŒ€ê´„í˜¸ê°€ ì—†ëŠ” ê²½ìš° ë‹¨ì¼ IPC ì½”ë“œë¡œ ì²˜ë¦¬
        codes = [ipc_str.strip()]
    
    # ì½”ë“œ ê¸¸ì´ì— ë”°ë¼ ë³€í™˜
    shortened_codes = []
    for code in codes:
        if code_length == 4:
            # B66B-001/34 -> B66B
            match = re.match(r'([A-Z]\d{2}[A-Z])', code)
            if match:
                shortened_codes.append(match.group(1))
        elif code_length == 8:
            # B66B-001/34 -> B66B-001
            match = re.match(r'([A-Z]\d{2}[A-Z]-\d{3})', code)
            if match:
                shortened_codes.append(match.group(1))
    
    return shortened_codes

def extract_entities_from_delimited_string(entity_str, delimiter="|"):
    """
    êµ¬ë¶„ìë¡œ ë¶„ë¦¬ëœ ë¬¸ìì—´ì—ì„œ ê°œë³„ ì—”í‹°í‹°ë“¤ì„ ì¶”ì¶œí•©ë‹ˆë‹¤.
    (ë°œëª…ì, ì¶œì›ì¸ ë“±ì— ì‚¬ìš©)
    """
    if pd.isna(entity_str) or entity_str == '':
        return []
    
    # ë¬¸ìì—´ë¡œ ë³€í™˜
    entity_str = str(entity_str)
    
    # êµ¬ë¶„ìë¡œ ë¶„ë¦¬í•˜ê³  ê³µë°± ì œê±°
    entities = [entity.strip() for entity in entity_str.split(delimiter)]
    
    # ë¹ˆ ë¬¸ìì—´ ì œê±°
    entities = [entity for entity in entities if entity]
    
    return entities

def calculate_centrality_measures(edges_df, nodes_df):
    """
    ë„¤íŠ¸ì›Œí¬ ì¤‘ì‹¬ì„± ì§€í‘œë“¤ì„ ê³„ì‚°í•©ë‹ˆë‹¤.
    """
    # NetworkX ê·¸ë˜í”„ ìƒì„±
    G = nx.Graph()
    
    # ë…¸ë“œ ì¶”ê°€
    for _, node in nodes_df.iterrows():
        G.add_node(node['id'], name=node['Name'], label=node['Label'])
    
    # ì—£ì§€ ì¶”ê°€ (ê°€ì¤‘ì¹˜ í¬í•¨)
    for _, edge in edges_df.iterrows():
        G.add_edge(edge['Source'], edge['Target'], weight=edge['Weight'])
    
    # ì—°ê²°ëœ ì»´í¬ë„ŒíŠ¸ë§Œ ë¶„ì„ (ê³ ë¦½ëœ ë…¸ë“œ ì œì™¸)
    if G.number_of_edges() == 0:
        # ì—£ì§€ê°€ ì—†ëŠ” ê²½ìš° ëª¨ë“  ì¤‘ì‹¬ì„±ì„ 0ìœ¼ë¡œ ì„¤ì •
        centrality_dict = {
            'EC': {node: 0.0 for node in G.nodes()},
            'BC': {node: 0.0 for node in G.nodes()},
            'CC': {node: 0.0 for node in G.nodes()}
        }
    else:
        # ê°€ì¥ í° ì—°ê²° ì»´í¬ë„ŒíŠ¸ ì„ íƒ
        largest_cc = max(nx.connected_components(G), key=len)
        G_connected = G.subgraph(largest_cc)
        
        # ì¤‘ì‹¬ì„± ì§€í‘œ ê³„ì‚°
        try:
            # Eigenvector Centrality (ê³ ìœ ë²¡í„° ì¤‘ì‹¬ì„±)
            ec = nx.eigenvector_centrality(G_connected, weight='weight', max_iter=1000)
        except:
            # ìˆ˜ë ´í•˜ì§€ ì•ŠëŠ” ê²½ìš° ê· ë“±í•˜ê²Œ ë¶„ë°°
            ec = {node: 1.0/len(G_connected) for node in G_connected.nodes()}
        
        try:
            # Betweenness Centrality (ë§¤ê°œ ì¤‘ì‹¬ì„±)
            bc = nx.betweenness_centrality(G_connected, weight='weight')
        except:
            bc = {node: 0.0 for node in G_connected.nodes()}
        
        try:
            # Closeness Centrality (ê·¼ì ‘ ì¤‘ì‹¬ì„±)
            cc = nx.closeness_centrality(G_connected, distance='weight')
        except:
            cc = {node: 0.0 for node in G_connected.nodes()}
        
        # ì—°ê²°ë˜ì§€ ì•Šì€ ë…¸ë“œë“¤ì— ëŒ€í•´ 0 ê°’ í• ë‹¹
        centrality_dict = {
            'EC': {node: ec.get(node, 0.0) for node in G.nodes()},
            'BC': {node: bc.get(node, 0.0) for node in G.nodes()},
            'CC': {node: cc.get(node, 0.0) for node in G.nodes()}
        }
    
    # ë…¸ë“œ ë°ì´í„°í”„ë ˆì„ì— ì¤‘ì‹¬ì„± ì§€í‘œ ì¶”ê°€
    nodes_with_centrality = nodes_df.copy()
    nodes_with_centrality['EC'] = nodes_with_centrality['id'].map(centrality_dict['EC'])
    nodes_with_centrality['BC'] = nodes_with_centrality['id'].map(centrality_dict['BC'])
    nodes_with_centrality['CC'] = nodes_with_centrality['id'].map(centrality_dict['CC'])
    
    # Degree ê³„ì‚° (ì—°ê²° ìˆ˜)
    degree_dict = dict(G.degree())
    nodes_with_centrality['Degree'] = nodes_with_centrality['id'].map(degree_dict)
    
    # Weighted Degree ê³„ì‚° (ê°€ì¤‘ì¹˜ í•©)
    weighted_degree_dict = dict(G.degree(weight='weight'))
    nodes_with_centrality['Weighted_Degree'] = nodes_with_centrality['id'].map(weighted_degree_dict)
    
    return nodes_with_centrality

# ìƒˆë¡œ ì¶”ê°€ëœ ë„¤íŠ¸ì›Œí¬ ì‹œê°í™” í•¨ìˆ˜ë“¤
def detect_communities(G):
    """
    ì»¤ë®¤ë‹ˆí‹° íƒì§€ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.
    """
    try:
        import networkx.algorithms.community as nx_comm
        communities = list(nx_comm.greedy_modularity_communities(G))
        
        # ë…¸ë“œë³„ ì»¤ë®¤ë‹ˆí‹° ID ë§¤í•‘
        node_community = {}
        for i, community in enumerate(communities):
            for node in community:
                node_community[node] = i
        
        return node_community, communities
    except:
        # ì»¤ë®¤ë‹ˆí‹° íƒì§€ ì‹¤íŒ¨ ì‹œ ëª¨ë“  ë…¸ë“œë¥¼ í•˜ë‚˜ì˜ ì»¤ë®¤ë‹ˆí‹°ë¡œ ì„¤ì •
        return {node: 0 for node in G.nodes()}, [set(G.nodes())]

def create_interactive_network_graph(edges_df, nodes_df, title="ë„¤íŠ¸ì›Œí¬ ê·¸ë˜í”„", 
                                   node_size_metric='Degree', node_color_metric='EC',
                                   max_nodes=100, min_edge_weight=1):
    """
    ì¸í„°ë™í‹°ë¸Œ ë„¤íŠ¸ì›Œí¬ ê·¸ë˜í”„ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    """
    if len(edges_df) == 0 or len(nodes_df) == 0:
        st.warning("ì‹œê°í™”í•  ë„¤íŠ¸ì›Œí¬ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return None
    
    # ì—£ì§€ ê°€ì¤‘ì¹˜ í•„í„°ë§
    filtered_edges = edges_df[edges_df['Weight'] >= min_edge_weight].copy()
    
    if len(filtered_edges) == 0:
        st.warning(f"ê°€ì¤‘ì¹˜ {min_edge_weight} ì´ìƒì¸ ì—£ì§€ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return None
    
    # í•„í„°ë§ëœ ì—£ì§€ì— ì—°ê²°ëœ ë…¸ë“œë§Œ ì„ íƒ
    connected_nodes = set(filtered_edges['Source'].tolist() + filtered_edges['Target'].tolist())
    filtered_nodes = nodes_df[nodes_df['id'].isin(connected_nodes)].copy()
    
    # ë…¸ë“œ ìˆ˜ ì œí•œ
    if len(filtered_nodes) > max_nodes:
        # ì„ íƒëœ ì§€í‘œë¥¼ ê¸°ì¤€ìœ¼ë¡œ ìƒìœ„ ë…¸ë“œë§Œ ì„ íƒ
        if node_size_metric in filtered_nodes.columns:
            top_nodes = filtered_nodes.nlargest(max_nodes, node_size_metric)
        else:
            top_nodes = filtered_nodes.head(max_nodes)
        
        top_node_ids = set(top_nodes['id'].tolist())
        filtered_edges = filtered_edges[
            (filtered_edges['Source'].isin(top_node_ids)) & 
            (filtered_edges['Target'].isin(top_node_ids))
        ]
        filtered_nodes = top_nodes
    
    # NetworkX ê·¸ë˜í”„ ìƒì„±
    G = nx.Graph()
    
    # ë…¸ë“œ ì¶”ê°€
    for _, node in filtered_nodes.iterrows():
        G.add_node(node['id'], **node.to_dict())
    
    # ì—£ì§€ ì¶”ê°€
    for _, edge in filtered_edges.iterrows():
        if edge['Source'] in G.nodes() and edge['Target'] in G.nodes():
            G.add_edge(edge['Source'], edge['Target'], weight=edge['Weight'])
    
    if len(G.nodes()) == 0:
        st.warning("í‘œì‹œí•  ë…¸ë“œê°€ ì—†ìŠµë‹ˆë‹¤.")
        return None
    
    # ì»¤ë®¤ë‹ˆí‹° íƒì§€
    node_community, communities = detect_communities(G)
    
    # ë ˆì´ì•„ì›ƒ ê³„ì‚°
    try:
        # Spring layout ì‚¬ìš© (ë…¸ë“œê°€ ë§ìœ¼ë©´ ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦´ ìˆ˜ ìˆìŒ)
        if len(G.nodes()) <= 50:
            pos = nx.spring_layout(G, k=3, iterations=50, seed=42)
        else:
            pos = nx.spring_layout(G, k=1, iterations=30, seed=42)
    except:
        # Fallbackìœ¼ë¡œ random layout ì‚¬ìš©
        pos = nx.random_layout(G, seed=42)
    
    # ë…¸ë“œ ìœ„ì¹˜ ë° ì†ì„± ì¶”ì¶œ
    node_x = []
    node_y = []
    node_text = []
    node_info = []
    node_sizes = []
    node_colors = []
    
    for node_id in G.nodes():
        x, y = pos[node_id]
        node_x.append(x)
        node_y.append(y)
        
        node_data = G.nodes[node_id]
        name = node_data.get('Name', str(node_id))
        label = node_data.get('Label', name)
        
        # í˜¸ë²„ ì •ë³´ ìƒì„±
        hover_info = f"<b>{name}</b><br>"
        hover_info += f"Label: {label}<br>"
        hover_info += f"Community: {node_community.get(node_id, 0)}<br>"
        
        for metric in ['EC', 'BC', 'CC', 'Degree', 'Weighted_Degree']:
            if metric in node_data:
                hover_info += f"{metric}: {node_data[metric]:.4f}<br>"
        
        node_text.append(label[:20] + '...' if len(label) > 20 else label)
        node_info.append(hover_info)
        
        # ë…¸ë“œ í¬ê¸° ì„¤ì •
        if node_size_metric in node_data and pd.notna(node_data[node_size_metric]):
            size_value = node_data[node_size_metric]
        else:
            size_value = 1
        node_sizes.append(max(size_value * 50, 5))  # ìµœì†Œ í¬ê¸° 5
        
        # ë…¸ë“œ ìƒ‰ìƒ ì„¤ì •
        if node_color_metric in node_data and pd.notna(node_data[node_color_metric]):
            color_value = node_data[node_color_metric]
        else:
            color_value = node_community.get(node_id, 0)
        node_colors.append(color_value)
    
    # ì—£ì§€ ìœ„ì¹˜ ì¶”ì¶œ
    edge_x = []
    edge_y = []
    edge_info = []
    edge_weights = []
    
    for edge in G.edges():
        x0, y0 = pos[edge[0]]
        x1, y1 = pos[edge[1]]
        edge_x.extend([x0, x1, None])
        edge_y.extend([y0, y1, None])
        
        weight = G.edges[edge].get('weight', 1)
        edge_weights.append(weight)
        
        # ì—£ì§€ ì •ë³´
        source_name = G.nodes[edge[0]].get('Name', str(edge[0]))
        target_name = G.nodes[edge[1]].get('Name', str(edge[1]))
        edge_info.append(f"{source_name} â†” {target_name}<br>Weight: {weight}")
    
    # Plotly ê·¸ë˜í”„ ìƒì„±
    fig = go.Figure()
    
    # ì—£ì§€ ì¶”ê°€
    fig.add_trace(go.Scatter(
        x=edge_x, y=edge_y,
        line=dict(width=0.5, color='rgba(125,125,125,0.5)'),
        hoverinfo='none',
        mode='lines',
        name='ì—°ê²°'
    ))
    
    # ë…¸ë“œ ì¶”ê°€
    fig.add_trace(go.Scatter(
        x=node_x, y=node_y,
        mode='markers+text',
        hoverinfo='text',
        text=node_text,
        textposition="middle center",
        hovertext=node_info,
        marker=dict(
            size=node_sizes,
            color=node_colors,
            colorscale='Viridis',
            colorbar=dict(
                title=dict(text=node_color_metric, side="right"),
                tickmode="linear"
            ),
            line=dict(width=0.5, color='rgba(50,50,50,0.5)')
        ),
        name='ë…¸ë“œ'
    ))
    
    # ë ˆì´ì•„ì›ƒ ì„¤ì •
    fig.update_layout(
        title=dict(
            text=title,
            x=0.5,
            font=dict(size=16)
        ),
        showlegend=False,
        hovermode='closest',
        margin=dict(b=20,l=5,r=5,t=40),
        annotations=[
            dict(
                text=f"ë…¸ë“œ ìˆ˜: {len(G.nodes())}, ì—£ì§€ ìˆ˜: {len(G.edges())}<br>" +
                     f"ë…¸ë“œ í¬ê¸°: {node_size_metric}, ë…¸ë“œ ìƒ‰ìƒ: {node_color_metric}",
                showarrow=False,
                xref="paper", yref="paper",
                x=0.005, y=-0.002,
                xanchor='left', yanchor='bottom',
                font=dict(size=10)
            )
        ],
        xaxis=dict(showgrid=False, zeroline=False, showticklabels=False),
        yaxis=dict(showgrid=False, zeroline=False, showticklabels=False),
        height=600
    )
    
    return fig

def create_network_comparison_graph(results_dict, analysis_type, node_size_metric='Degree', 
                                  node_color_metric='EC', max_nodes=50):
    """
    ì—¬ëŸ¬ ê¸°ê°„ì˜ ë„¤íŠ¸ì›Œí¬ë¥¼ ë¹„êµ ì‹œê°í™”í•©ë‹ˆë‹¤.
    """
    if len(results_dict) <= 1:
        st.warning("ë¹„êµí•  ê¸°ê°„ì´ ë¶€ì¡±í•©ë‹ˆë‹¤.")
        return None
    
    period_names = list(results_dict.keys())
    n_periods = len(period_names)
    
    # ì„œë¸Œí”Œë¡¯ ì„¤ì •
    if n_periods == 2:
        rows, cols = 1, 2
    elif n_periods == 3:
        rows, cols = 1, 3
    elif n_periods == 4:
        rows, cols = 2, 2
    else:
        rows, cols = 2, 3
    
    fig = make_subplots(
        rows=rows, cols=cols,
        subplot_titles=period_names,
        specs=[[{"type": "scatter"}] * cols for _ in range(rows)]
    )
    
    colors = px.colors.qualitative.Set1
    
    for idx, (period_name, result) in enumerate(results_dict.items()):
        row = idx // cols + 1
        col = idx % cols + 1
        
        edges_df = result['edges']
        nodes_df = result['nodes']
        
        if len(edges_df) == 0 or len(nodes_df) == 0:
            continue
        
        # ìƒìœ„ ë…¸ë“œë§Œ ì„ íƒ
        if len(nodes_df) > max_nodes:
            if node_size_metric in nodes_df.columns:
                top_nodes = nodes_df.nlargest(max_nodes, node_size_metric)
            else:
                top_nodes = nodes_df.head(max_nodes)
            
            top_node_ids = set(top_nodes['id'].tolist())
            filtered_edges = edges_df[
                (edges_df['Source'].isin(top_node_ids)) & 
                (edges_df['Target'].isin(top_node_ids))
            ]
            filtered_nodes = top_nodes
        else:
            filtered_edges = edges_df
            filtered_nodes = nodes_df
        
        # NetworkX ê·¸ë˜í”„ ìƒì„±
        G = nx.Graph()
        
        for _, node in filtered_nodes.iterrows():
            G.add_node(node['id'], **node.to_dict())
        
        for _, edge in filtered_edges.iterrows():
            if edge['Source'] in G.nodes() and edge['Target'] in G.nodes():
                G.add_edge(edge['Source'], edge['Target'], weight=edge['Weight'])
        
        if len(G.nodes()) == 0:
            continue
        
        # ë ˆì´ì•„ì›ƒ ê³„ì‚°
        try:
            pos = nx.spring_layout(G, k=2, iterations=30, seed=42)
        except:
            pos = nx.random_layout(G, seed=42)
        
        # ì—£ì§€ ê·¸ë¦¬ê¸°
        edge_x = []
        edge_y = []
        for edge in G.edges():
            x0, y0 = pos[edge[0]]
            x1, y1 = pos[edge[1]]
            edge_x.extend([x0, x1, None])
            edge_y.extend([y0, y1, None])
        
        fig.add_trace(go.Scatter(
            x=edge_x, y=edge_y,
            line=dict(width=0.5, color='rgba(125,125,125,0.3)'),
            hoverinfo='none',
            mode='lines',
            showlegend=False
        ), row=row, col=col)
        
        # ë…¸ë“œ ê·¸ë¦¬ê¸°
        node_x = []
        node_y = []
        node_sizes = []
        node_colors = []
        node_text = []
        
        for node_id in G.nodes():
            x, y = pos[node_id]
            node_x.append(x)
            node_y.append(y)
            
            node_data = G.nodes[node_id]
            name = node_data.get('Name', str(node_id))
            node_text.append(name[:10] + '...' if len(name) > 10 else name)
            
            # ë…¸ë“œ í¬ê¸°
            if node_size_metric in node_data and pd.notna(node_data[node_size_metric]):
                size_value = node_data[node_size_metric]
            else:
                size_value = 1
            node_sizes.append(max(size_value * 30, 3))
            
            # ë…¸ë“œ ìƒ‰ìƒ
            if node_color_metric in node_data and pd.notna(node_data[node_color_metric]):
                color_value = node_data[node_color_metric]
            else:
                color_value = 0
            node_colors.append(color_value)
        
        fig.add_trace(go.Scatter(
            x=node_x, y=node_y,
            mode='markers+text',
            text=node_text,
            textposition="middle center",
            marker=dict(
                size=node_sizes,
                color=node_colors,
                colorscale='Viridis',
                line=dict(width=0.5, color='rgba(50,50,50,0.5)')
            ),
            showlegend=False
        ), row=row, col=col)
        
        # ì¶• ì„¤ì •
        fig.update_xaxes(showgrid=False, zeroline=False, showticklabels=False, row=row, col=col)
        fig.update_yaxes(showgrid=False, zeroline=False, showticklabels=False, row=row, col=col)
    
    fig.update_layout(
        title=f"{analysis_type} êµ¬ê°„ë³„ ë„¤íŠ¸ì›Œí¬ ë¹„êµ",
        height=400 * rows,
        showlegend=False
    )
    
    return fig

def create_network_statistics_dashboard(results_dict, analysis_type):
    """
    ë„¤íŠ¸ì›Œí¬ í†µê³„ ëŒ€ì‹œë³´ë“œë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    """
    if not results_dict:
        return None
    
    # í†µê³„ ë°ì´í„° ìˆ˜ì§‘
    stats_data = []
    
    for period_name, result in results_dict.items():
        edges_df = result['edges']
        nodes_df = result['nodes']
        
        if len(nodes_df) == 0:
            continue
        
        # NetworkX ê·¸ë˜í”„ ìƒì„±
        G = nx.Graph()
        
        for _, node in nodes_df.iterrows():
            G.add_node(node['id'])
        
        for _, edge in edges_df.iterrows():
            if edge['Source'] in G.nodes() and edge['Target'] in G.nodes():
                G.add_edge(edge['Source'], edge['Target'], weight=edge['Weight'])
        
        # ë„¤íŠ¸ì›Œí¬ ê¸°ë³¸ í†µê³„
        num_nodes = len(G.nodes())
        num_edges = len(G.edges())
        density = nx.density(G) if num_nodes > 1 else 0
        
        # ì—°ê²°ì„± í†µê³„
        if num_edges > 0:
            components = list(nx.connected_components(G))
            largest_component_size = len(max(components, key=len))
            avg_clustering = nx.average_clustering(G)
            
            # ìµœë‹¨ ê²½ë¡œ ê¸¸ì´ (ê°€ì¥ í° ì»´í¬ë„ŒíŠ¸ì—ì„œë§Œ)
            if largest_component_size > 1:
                largest_cc = G.subgraph(max(components, key=len))
                try:
                    avg_path_length = nx.average_shortest_path_length(largest_cc)
                except:
                    avg_path_length = 0
            else:
                avg_path_length = 0
        else:
            largest_component_size = 0
            avg_clustering = 0
            avg_path_length = 0
        
        # ì¤‘ì‹¬ì„± í†µê³„
        if 'EC' in nodes_df.columns:
            avg_ec = nodes_df['EC'].mean()
            max_ec = nodes_df['EC'].max()
        else:
            avg_ec = max_ec = 0
        
        if 'BC' in nodes_df.columns:
            avg_bc = nodes_df['BC'].mean()
            max_bc = nodes_df['BC'].max()
        else:
            avg_bc = max_bc = 0
        
        if 'CC' in nodes_df.columns:
            avg_cc = nodes_df['CC'].mean()
            max_cc = nodes_df['CC'].max()
        else:
            avg_cc = max_cc = 0
        
        stats_data.append({
            'êµ¬ê°„': period_name,
            'ë…¸ë“œ ìˆ˜': num_nodes,
            'ì—£ì§€ ìˆ˜': num_edges,
            'ë°€ë„': density,
            'ìµœëŒ€ ì»´í¬ë„ŒíŠ¸ í¬ê¸°': largest_component_size,
            'í‰ê·  í´ëŸ¬ìŠ¤í„°ë§': avg_clustering,
            'í‰ê·  ìµœë‹¨ê²½ë¡œ': avg_path_length,
            'í‰ê·  EC': avg_ec,
            'ìµœëŒ€ EC': max_ec,
            'í‰ê·  BC': avg_bc,
            'ìµœëŒ€ BC': max_bc,
            'í‰ê·  CC': avg_cc,
            'ìµœëŒ€ CC': max_cc
        })
    
    if not stats_data:
        return None
    
    stats_df = pd.DataFrame(stats_data)
    
    # ì„œë¸Œí”Œë¡¯ ìƒì„±
    fig = make_subplots(
        rows=2, cols=3,
        subplot_titles=[
            'ë…¸ë“œ/ì—£ì§€ ìˆ˜', 'ë„¤íŠ¸ì›Œí¬ ë°€ë„', 'í´ëŸ¬ìŠ¤í„°ë§ ê³„ìˆ˜',
            'í‰ê·  ê³ ìœ ë²¡í„° ì¤‘ì‹¬ì„±', 'í‰ê·  ë§¤ê°œ ì¤‘ì‹¬ì„±', 'í‰ê·  ê·¼ì ‘ ì¤‘ì‹¬ì„±'
        ],
        specs=[[{"secondary_y": False}, {"secondary_y": False}, {"secondary_y": False}],
               [{"secondary_y": False}, {"secondary_y": False}, {"secondary_y": False}]]
    )
    
    # 1. ë…¸ë“œ/ì—£ì§€ ìˆ˜
    fig.add_trace(
        go.Bar(x=stats_df['êµ¬ê°„'], y=stats_df['ë…¸ë“œ ìˆ˜'], name='ë…¸ë“œ ìˆ˜', marker_color='lightblue'),
        row=1, col=1
    )
    fig.add_trace(
        go.Bar(x=stats_df['êµ¬ê°„'], y=stats_df['ì—£ì§€ ìˆ˜'], name='ì—£ì§€ ìˆ˜', marker_color='lightcoral'),
        row=1, col=1
    )
    
    # 2. ë„¤íŠ¸ì›Œí¬ ë°€ë„
    fig.add_trace(
        go.Scatter(x=stats_df['êµ¬ê°„'], y=stats_df['ë°€ë„'], mode='lines+markers', 
                  name='ë°€ë„', line=dict(color='green')),
        row=1, col=2
    )
    
    # 3. í´ëŸ¬ìŠ¤í„°ë§ ê³„ìˆ˜
    fig.add_trace(
        go.Scatter(x=stats_df['êµ¬ê°„'], y=stats_df['í‰ê·  í´ëŸ¬ìŠ¤í„°ë§'], mode='lines+markers',
                  name='í´ëŸ¬ìŠ¤í„°ë§', line=dict(color='purple')),
        row=1, col=3
    )
    
    # 4. í‰ê·  ê³ ìœ ë²¡í„° ì¤‘ì‹¬ì„±
    fig.add_trace(
        go.Bar(x=stats_df['êµ¬ê°„'], y=stats_df['í‰ê·  EC'], name='í‰ê·  EC', marker_color='orange'),
        row=2, col=1
    )
    
    # 5. í‰ê·  ë§¤ê°œ ì¤‘ì‹¬ì„±
    fig.add_trace(
        go.Bar(x=stats_df['êµ¬ê°„'], y=stats_df['í‰ê·  BC'], name='í‰ê·  BC', marker_color='red'),
        row=2, col=2
    )
    
    # 6. í‰ê·  ê·¼ì ‘ ì¤‘ì‹¬ì„±
    fig.add_trace(
        go.Bar(x=stats_df['êµ¬ê°„'], y=stats_df['í‰ê·  CC'], name='í‰ê·  CC', marker_color='blue'),
        row=2, col=3
    )
    
    fig.update_layout(
        title=f'{analysis_type} ë„¤íŠ¸ì›Œí¬ í†µê³„ ëŒ€ì‹œë³´ë“œ',
        showlegend=False,
        height=600
    )
    
    return fig, stats_df

def create_centrality_distribution_plot(nodes_df, title="ì¤‘ì‹¬ì„± ì§€í‘œ ë¶„í¬"):
    """
    ì¤‘ì‹¬ì„± ì§€í‘œë“¤ì˜ ë¶„í¬ë¥¼ ì‹œê°í™”í•©ë‹ˆë‹¤.
    """
    if len(nodes_df) == 0:
        return None
    
    centrality_metrics = ['EC', 'BC', 'CC']
    available_metrics = [metric for metric in centrality_metrics if metric in nodes_df.columns]
    
    if not available_metrics:
        st.warning("ì¤‘ì‹¬ì„± ì§€í‘œê°€ ì—†ìŠµë‹ˆë‹¤.")
        return None
    
    fig = make_subplots(
        rows=1, cols=len(available_metrics),
        subplot_titles=[f'{metric} ë¶„í¬' for metric in available_metrics]
    )
    
    colors = ['blue', 'red', 'green']
    
    for i, metric in enumerate(available_metrics):
        values = nodes_df[metric].dropna()
        
        if len(values) == 0:
            continue
        
        # íˆìŠ¤í† ê·¸ë¨
        fig.add_trace(
            go.Histogram(
                x=values,
                name=f'{metric} ë¶„í¬',
                marker_color=colors[i % len(colors)],
                opacity=0.7,
                nbinsx=20
            ),
            row=1, col=i+1
        )
        
        # í†µê³„ ì •ë³´ ì¶”ê°€
        mean_val = values.mean()
        median_val = values.median()
        
        fig.add_vline(
            x=mean_val, line_dash="dash", line_color="red",
            annotation_text=f"í‰ê· : {mean_val:.4f}",
            row=1, col=i+1
        )
        
        fig.add_vline(
            x=median_val, line_dash="dot", line_color="blue",
            annotation_text=f"ì¤‘ì•™ê°’: {median_val:.4f}",
            row=1, col=i+1
        )
    
    fig.update_layout(
        title=title,
        showlegend=False,
        height=400
    )
    
    return fig

def create_degree_distribution_plot(nodes_df, title="ì—°ê²°ë„ ë¶„í¬"):
    """
    ë…¸ë“œì˜ ì—°ê²°ë„ ë¶„í¬ë¥¼ ì‹œê°í™”í•©ë‹ˆë‹¤.
    """
    if len(nodes_df) == 0 or 'Degree' not in nodes_df.columns:
        return None
    
    degrees = nodes_df['Degree'].values
    degree_counts = Counter(degrees)
    
    x_vals = list(degree_counts.keys())
    y_vals = list(degree_counts.values())
    
    fig = go.Figure()
    
    # ë§‰ëŒ€ ê·¸ë˜í”„
    fig.add_trace(go.Bar(
        x=x_vals,
        y=y_vals,
        name='ì—°ê²°ë„ ë¶„í¬',
        marker_color='lightblue'
    ))
    
    # ë¡œê·¸ ìŠ¤ì¼€ì¼ ì˜µì…˜
    fig.add_trace(go.Scatter(
        x=x_vals,
        y=y_vals,
        mode='lines+markers',
        name='ì—°ê²°ë„ ë¶„í¬ (ì„ í˜•)',
        line=dict(color='red'),
        visible='legendonly'
    ))
    
    fig.update_layout(
        title=title,
        xaxis_title='ì—°ê²°ë„ (Degree)',
        yaxis_title='ë…¸ë“œ ìˆ˜',
        showlegend=True,
        height=400
    )
    
    return fig

def extract_network_insights(edges_df, nodes_df, analysis_type):
    """
    ë„¤íŠ¸ì›Œí¬ì—ì„œ ì£¼ìš” ì¸ì‚¬ì´íŠ¸ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
    """
    if len(nodes_df) == 0:
        return {}
    
    insights = {}
    
    # 1. ê°€ì¥ ì¤‘ìš”í•œ ë…¸ë“œë“¤ (ê° ì¤‘ì‹¬ì„± ê¸°ì¤€)
    centrality_metrics = ['EC', 'BC', 'CC', 'Degree', 'Weighted_Degree']
    
    for metric in centrality_metrics:
        if metric in nodes_df.columns:
            top_node = nodes_df.loc[nodes_df[metric].idxmax()]
            insights[f'top_{metric.lower()}'] = {
                'name': top_node['Name'],
                'label': top_node.get('Label', top_node['Name']),
                'value': top_node[metric]
            }
    
    # 2. ë„¤íŠ¸ì›Œí¬ êµ¬ì¡° íŠ¹ì„±
    if len(edges_df) > 0:
        # NetworkX ê·¸ë˜í”„ ìƒì„±
        G = nx.Graph()
        
        for _, node in nodes_df.iterrows():
            G.add_node(node['id'])
        
        for _, edge in edges_df.iterrows():
            G.add_edge(edge['Source'], edge['Target'], weight=edge['Weight'])
        
        # ê¸°ë³¸ í†µê³„
        insights['network_stats'] = {
            'nodes': len(G.nodes()),
            'edges': len(G.edges()),
            'density': nx.density(G),
            'components': nx.number_connected_components(G)
        }
        
        # ê°€ì¥ ê°•í•œ ì—°ê²°
        if len(edges_df) > 0:
            strongest_edge = edges_df.loc[edges_df['Weight'].idxmax()]
            source_name = nodes_df.loc[nodes_df['id'] == strongest_edge['Source'], 'Name'].iloc[0]
            target_name = nodes_df.loc[nodes_df['id'] == strongest_edge['Target'], 'Name'].iloc[0]
            
            insights['strongest_connection'] = {
                'source': source_name,
                'target': target_name,
                'weight': strongest_edge['Weight']
            }
    
    return insights

def calculate_timeseries_stats(df, date_column, entity_column, entity_type="Entity"):
    """
    ì‹œê³„ì—´ í†µê³„ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.
    """
    if date_column not in df.columns or entity_column not in df.columns:
        return pd.DataFrame()
    
    # ë‚ ì§œ íŒŒì‹±
    df_temp = df.copy()
    df_temp['parsed_date'] = df_temp[date_column].apply(parse_application_date)
    
    # ìœ íš¨í•œ ë‚ ì§œê°€ ìˆëŠ” í–‰ë§Œ í•„í„°ë§ (Noneì´ ì•„ë‹Œ ê°’ë“¤ë§Œ)
    df_temp = df_temp[df_temp['parsed_date'].notna()]
    
    if len(df_temp) == 0:
        st.warning(f"ìœ íš¨í•œ ë‚ ì§œ ë°ì´í„°ê°€ ì—†ì–´ ì‹œê³„ì—´ ë¶„ì„ì„ ìˆ˜í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return pd.DataFrame()
    
    # datetime íƒ€ì…ìœ¼ë¡œ ë³€í™˜
    df_temp['parsed_date'] = pd.to_datetime(df_temp['parsed_date'])
    df_temp['year'] = df_temp['parsed_date'].dt.year
    
    # ì—°ë„ë³„ í†µê³„
    yearly_stats = []
    
    for year in sorted(df_temp['year'].unique()):
        year_data = df_temp[df_temp['year'] == year]
        
        # ì—”í‹°í‹° ì¶”ì¶œ
        all_entities = []
        for _, row in year_data.iterrows():
            if entity_type == "IPC":
                entities = extract_shortened_ipc_codes(row[entity_column], code_length=4)
            else:
                entities = extract_entities_from_delimited_string(row[entity_column])
            all_entities.extend(entities)
        
        unique_entities = len(set(all_entities))
        total_patents = len(year_data)
        
        yearly_stats.append({
            'Year': year,
            'Total_Patents': total_patents,
            f'Unique_{entity_type}': unique_entities,
            f'Avg_{entity_type}_per_Patent': len(all_entities) / total_patents if total_patents > 0 else 0
        })
    
    return pd.DataFrame(yearly_stats)

def create_timeseries_visualization(stats_df, entity_type, periods_info=None):
    """
    ì‹œê³„ì—´ ì‹œê°í™”ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    """
    if stats_df.empty:
        return None
    
    fig = make_subplots(
        rows=2, cols=2,
        subplot_titles=(
            f'ì—°ë„ë³„ íŠ¹í—ˆ ê±´ìˆ˜',
            f'ì—°ë„ë³„ ê³ ìœ  {entity_type} ìˆ˜',
            f'íŠ¹í—ˆë‹¹ í‰ê·  {entity_type} ìˆ˜',
            f'ëˆ„ì  íŠ¹í—ˆ ê±´ìˆ˜'
        ),
        specs=[[{"secondary_y": False}, {"secondary_y": False}],
               [{"secondary_y": False}, {"secondary_y": False}]]
    )
    
    # 1. ì—°ë„ë³„ íŠ¹í—ˆ ê±´ìˆ˜
    fig.add_trace(
        go.Scatter(x=stats_df['Year'], y=stats_df['Total_Patents'], 
                  mode='lines+markers', name='íŠ¹í—ˆ ê±´ìˆ˜', line=dict(color='blue')),
        row=1, col=1
    )
    
    # 2. ì—°ë„ë³„ ê³ ìœ  ì—”í‹°í‹° ìˆ˜
    fig.add_trace(
        go.Scatter(x=stats_df['Year'], y=stats_df[f'Unique_{entity_type}'], 
                  mode='lines+markers', name=f'ê³ ìœ  {entity_type}', line=dict(color='green')),
        row=1, col=2
    )
    
    # 3. íŠ¹í—ˆë‹¹ í‰ê·  ì—”í‹°í‹° ìˆ˜
    fig.add_trace(
        go.Scatter(x=stats_df['Year'], y=stats_df[f'Avg_{entity_type}_per_Patent'], 
                  mode='lines+markers', name=f'í‰ê·  {entity_type}', line=dict(color='red')),
        row=2, col=1
    )
    
    # 4. ëˆ„ì  íŠ¹í—ˆ ê±´ìˆ˜
    cumulative_patents = stats_df['Total_Patents'].cumsum()
    fig.add_trace(
        go.Scatter(x=stats_df['Year'], y=cumulative_patents, 
                  mode='lines+markers', name='ëˆ„ì  íŠ¹í—ˆ', line=dict(color='purple')),
        row=2, col=2
    )
    
    # êµ¬ê°„ í‘œì‹œ (ìˆëŠ” ê²½ìš°)
    if periods_info:
        colors = ['rgba(255,0,0,0.2)', 'rgba(0,255,0,0.2)', 'rgba(0,0,255,0.2)']
        for i, period in enumerate(periods_info):
            if i < len(colors):
                for row in range(1, 3):
                    for col in range(1, 3):
                        fig.add_vrect(
                            x0=period['start'], x1=period['end'],
                            fillcolor=colors[i], opacity=0.3,
                            layer="below", line_width=0,
                            row=row, col=col
                        )
    
    fig.update_layout(
        title=f'{entity_type} ì‹œê³„ì—´ ë¶„ì„',
        showlegend=True,
        height=600
    )
    
    return fig

def calculate_yearly_centrality_analysis(df, entity_column, entity_type, code_length=None):
    """
    ì—°ë„ë³„ ì¤‘ì‹¬ì„± ì§€í‘œë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.
    """
    # ë‚ ì§œ íŒŒì‹±
    df_temp = df.copy()
    df_temp['parsed_date'] = df_temp['ì¶œì›ì¼'].apply(parse_application_date)
    df_temp = df_temp[df_temp['parsed_date'].notna()]
    
    if len(df_temp) == 0:
        return {}
    
    df_temp['parsed_date'] = pd.to_datetime(df_temp['parsed_date'])
    df_temp['year'] = df_temp['parsed_date'].dt.year
    
    yearly_results = {}
    
    for year in sorted(df_temp['year'].unique()):
        year_data = df_temp[df_temp['year'] == year]
        
        if entity_type == "IPC":
            edges, nodes = calculate_single_period_ipc(year_data, entity_column, code_length, f"{year}ë…„")
        else:
            edges, nodes = calculate_single_period_entity(year_data, entity_column, entity_type, f"{year}ë…„")
        
        # ì¤‘ì‹¬ì„± ì§€í‘œê°€ ìˆëŠ”ì§€ í™•ì¸í•˜ê³ , í•„ìš”í•œ ì»¬ëŸ¼ë§Œ ì„ íƒ
        if len(nodes) > 0:
            available_columns = ['Name']
            
            # ì¤‘ì‹¬ì„± ì§€í‘œ ì»¬ëŸ¼ë“¤ì´ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
            for col in ['EC', 'BC', 'CC']:
                if col in nodes.columns:
                    available_columns.append(col)
            
            if len(available_columns) > 1:  # Name ì™¸ì— ë‹¤ë¥¸ ì»¬ëŸ¼ì´ ìˆëŠ” ê²½ìš°
                yearly_results[year] = nodes[available_columns].copy()
                yearly_results[year]['Year'] = year
    
    return yearly_results

def create_centrality_trend_visualization(yearly_results, entity_type, top_n=5):
    """
    ìƒìœ„ ë…¸ë“œë“¤ì˜ ì¤‘ì‹¬ì„± ì§€í‘œ ì¶”ì´ë¥¼ ì‹œê°í™”í•©ë‹ˆë‹¤.
    """
    if not yearly_results:
        return None
    
    # ì „ì²´ ê¸°ê°„ì—ì„œ ê° ì¤‘ì‹¬ì„± ì§€í‘œì˜ ìƒìœ„ ë…¸ë“œë“¤ ì‹ë³„
    all_nodes_centrality = {}
    
    for year, nodes_df in yearly_results.items():
        for _, row in nodes_df.iterrows():
            node_name = row['Name']
            if node_name not in all_nodes_centrality:
                all_nodes_centrality[node_name] = {'EC': [], 'BC': [], 'CC': [], 'years': []}
            
            # ì¤‘ì‹¬ì„± ì§€í‘œê°€ ìˆëŠ” ê²½ìš°ì—ë§Œ ì¶”ê°€
            if 'EC' in row:
                all_nodes_centrality[node_name]['EC'].append(row['EC'])
            else:
                all_nodes_centrality[node_name]['EC'].append(0.0)
                
            if 'BC' in row:
                all_nodes_centrality[node_name]['BC'].append(row['BC'])
            else:
                all_nodes_centrality[node_name]['BC'].append(0.0)
                
            if 'CC' in row:
                all_nodes_centrality[node_name]['CC'].append(row['CC'])
            else:
                all_nodes_centrality[node_name]['CC'].append(0.0)
            
            all_nodes_centrality[node_name]['years'].append(year)
    
    # ê° ì¤‘ì‹¬ì„± ì§€í‘œë³„ë¡œ í‰ê· ê°’ ê³„ì‚°í•˜ì—¬ ìƒìœ„ ë…¸ë“œ ì„ ì •
    top_nodes = {'EC': [], 'BC': [], 'CC': []}
    
    for metric in ['EC', 'BC', 'CC']:
        node_avg_centrality = []
        for node_name, data in all_nodes_centrality.items():
            if data[metric]:
                avg_centrality = sum(data[metric]) / len(data[metric])
                node_avg_centrality.append((node_name, avg_centrality))
        
        # ìƒìœ„ Nê°œ ë…¸ë“œ ì„ ì •
        node_avg_centrality.sort(key=lambda x: x[1], reverse=True)
        top_nodes[metric] = [node[0] for node in node_avg_centrality[:top_n]]
    
    # ì‹œê°í™” ìƒì„±
    fig = make_subplots(
        rows=1, cols=3,
        subplot_titles=('Eigenvector Centrality ì¶”ì´', 'Betweenness Centrality ì¶”ì´', 'Closeness Centrality ì¶”ì´'),
        specs=[[{"secondary_y": False}, {"secondary_y": False}, {"secondary_y": False}]]
    )
    
    colors = px.colors.qualitative.Set1
    
    for col, metric in enumerate(['EC', 'BC', 'CC'], 1):
        for i, node_name in enumerate(top_nodes[metric]):
            if node_name in all_nodes_centrality:
                years = all_nodes_centrality[node_name]['years']
                values = all_nodes_centrality[node_name][metric]
                
                # ë…¸ë“œëª… ë‹¨ì¶•
                display_name = f'{node_name[:20]}...' if len(node_name) > 20 else node_name
                
                # ê° ì„œë¸Œí”Œë¡¯ë³„ë¡œ ê³ ìœ í•œ trace ì´ë¦„ ìƒì„±
                trace_name = f"{display_name} ({metric})"
                
                fig.add_trace(
                    go.Scatter(
                        x=years, 
                        y=values,
                        mode='lines+markers',
                        name=trace_name,  # ì„œë¸Œí”Œë¡¯ë³„ ê³ ìœ  ì´ë¦„
                        line=dict(color=colors[i % len(colors)]),
                        showlegend=True  # ëª¨ë“  traceì— ë²”ë¡€ í‘œì‹œ
                    ),
                    row=1, col=col
                )
    
    fig.update_layout(
        title=f'{entity_type} ìƒìœ„ {top_n}ê°œ ë…¸ë“œì˜ ì¤‘ì‹¬ì„± ì§€í‘œ ì¶”ì´',
        height=500,
        showlegend=True,
        legend=dict(
            orientation="v",
            yanchor="top",
            y=1,
            xanchor="left",
            x=1.05
        ),
        margin=dict(r=150)  # ë²”ë¡€ë¥¼ ìœ„í•œ ì˜¤ë¥¸ìª½ ì—¬ë°± ì¶”ê°€
    )
    
    return fig

def analyze_period_changes(period_results, analysis_type):
    """
    êµ¬ê°„ë³„ ë…¸ë“œ ë³€í™”ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤.
    """
    if len(period_results) < 2:
        return None, None
    
    period_names = [name for name in period_results.keys() if name != "ì „ì²´ ê¸°ê°„"]
    
    # êµ¬ê°„ë³„ ë…¸ë“œ ì§‘í•© ìƒì„±
    period_nodes = {}
    period_centrality = {}
    
    for period_name, result in period_results.items():
        nodes_df = result['nodes']
        if len(nodes_df) > 0:
            period_nodes[period_name] = set(nodes_df['Name'].tolist())
            period_centrality[period_name] = nodes_df.set_index('Name')[['EC', 'BC', 'CC']].to_dict('index')
        else:
            period_nodes[period_name] = set()
            period_centrality[period_name] = {}
    
    # ì‹ ê·œ/ì†Œë©¸ ë…¸ë“œ ë¶„ì„
    changes_analysis = {}
    
    for i in range(1, len(period_names)):
        prev_period = period_names[i-1]
        curr_period = period_names[i]
        
        prev_nodes = period_nodes[prev_period]
        curr_nodes = period_nodes[curr_period]
        
        new_nodes = curr_nodes - prev_nodes
        disappeared_nodes = prev_nodes - curr_nodes
        common_nodes = prev_nodes & curr_nodes
        
        changes_analysis[f"{prev_period} â†’ {curr_period}"] = {
            'new_nodes': list(new_nodes),
            'disappeared_nodes': list(disappeared_nodes),
            'common_nodes': list(common_nodes)
        }
    
    # ì¤‘ì‹¬ì„± ì§€í‘œ ë³€í™” ë¶„ì„
    centrality_changes = {}
    
    for i in range(1, len(period_names)):
        prev_period = period_names[i-1]
        curr_period = period_names[i]
        
        prev_centrality = period_centrality[prev_period]
        curr_centrality = period_centrality[curr_period]
        
        common_nodes = set(prev_centrality.keys()) & set(curr_centrality.keys())
        
        node_changes = []
        for node in common_nodes:
            prev_data = prev_centrality[node]
            curr_data = curr_centrality[node]
            
            ec_change = curr_data['EC'] - prev_data['EC']
            bc_change = curr_data['BC'] - prev_data['BC']
            cc_change = curr_data['CC'] - prev_data['CC']
            
            node_changes.append({
                'Node': node,
                'EC_Change': ec_change,
                'BC_Change': bc_change,
                'CC_Change': cc_change,
                'EC_Prev': prev_data['EC'],
                'EC_Curr': curr_data['EC'],
                'BC_Prev': prev_data['BC'],
                'BC_Curr': curr_data['BC'],
                'CC_Prev': prev_data['CC'],
                'CC_Curr': curr_data['CC']
            })
        
        centrality_changes[f"{prev_period} â†’ {curr_period}"] = pd.DataFrame(node_changes)
    
    return changes_analysis, centrality_changes

def display_period_changes_analysis(changes_analysis, centrality_changes, analysis_type):
    """
    êµ¬ê°„ë³„ ë³€í™” ë¶„ì„ ê²°ê³¼ë¥¼ í‘œì‹œí•©ë‹ˆë‹¤.
    """
    if not changes_analysis or not centrality_changes:
        st.warning("êµ¬ê°„ë³„ ë³€í™” ë¶„ì„ì„ ìœ„í•´ì„œëŠ” ìµœì†Œ 2ê°œ ì´ìƒì˜ êµ¬ê°„ì´ í•„ìš”í•©ë‹ˆë‹¤.")
        return
    
    st.header(f"ğŸ“ˆ {analysis_type} êµ¬ê°„ë³„ ë³€í™” ë¶„ì„")
    
    for transition, changes in changes_analysis.items():
        st.subheader(f"ğŸ“Š {transition} ë³€í™”")
        
        col1, col2, col3 = st.columns(3)
        
        with col1:
            st.metric("ì‹ ê·œ ë“±ì¥", len(changes['new_nodes']))
            if changes['new_nodes']:
                with st.expander(f"ì‹ ê·œ ë“±ì¥ {analysis_type} ë³´ê¸°"):
                    for node in changes['new_nodes'][:20]:  # ìƒìœ„ 20ê°œë§Œ í‘œì‹œ
                        st.write(f"â€¢ {node}")
                    if len(changes['new_nodes']) > 20:
                        st.write(f"... ì™¸ {len(changes['new_nodes']) - 20}ê°œ")
        
        with col2:
            st.metric("ì†Œë©¸", len(changes['disappeared_nodes']))
            if changes['disappeared_nodes']:
                with st.expander(f"ì†Œë©¸ëœ {analysis_type} ë³´ê¸°"):
                    for node in changes['disappeared_nodes'][:20]:  # ìƒìœ„ 20ê°œë§Œ í‘œì‹œ
                        st.write(f"â€¢ {node}")
                    if len(changes['disappeared_nodes']) > 20:
                        st.write(f"... ì™¸ {len(changes['disappeared_nodes']) - 20}ê°œ")
        
        with col3:
            st.metric("ì§€ì†", len(changes['common_nodes']))
        
        # ì¤‘ì‹¬ì„± ì§€í‘œ ë³€í™” ë¶„ì„
        if transition in centrality_changes:
            centrality_df = centrality_changes[transition]
            
            if len(centrality_df) > 0:
                st.subheader(f"ğŸ”„ {transition} ì¤‘ì‹¬ì„± ì§€í‘œ ë³€í™”")
                
                # ê° ì¤‘ì‹¬ì„± ì§€í‘œë³„ ìƒìœ„/í•˜ìœ„ ë³€í™” ë…¸ë“œë“¤
                col1, col2, col3 = st.columns(3)
                
                with col1:
                    st.write("**EC (ê³ ìœ ë²¡í„° ì¤‘ì‹¬ì„±) ë³€í™”**")
                    
                    # ì‹¤ì œ ìƒìŠ¹í•œ ë…¸ë“œë“¤ë§Œ í•„í„°ë§
                    ec_increased = centrality_df[centrality_df['EC_Change'] > 0].nlargest(5, 'EC_Change')[['Node', 'EC_Change', 'EC_Prev', 'EC_Curr']]
                    # ì‹¤ì œ í•˜ë½í•œ ë…¸ë“œë“¤ë§Œ í•„í„°ë§
                    ec_decreased = centrality_df[centrality_df['EC_Change'] < 0].nsmallest(5, 'EC_Change')[['Node', 'EC_Change', 'EC_Prev', 'EC_Curr']]
                    
                    st.write("ìƒìŠ¹ TOP 5:")
                    if len(ec_increased) > 0:
                        for _, row in ec_increased.iterrows():
                            st.write(f"ğŸ“ˆ {row['Node']}")
                            st.write(f"   {row['EC_Prev']:.4f} â†’ {row['EC_Curr']:.4f} (ë³€í™”: +{row['EC_Change']:.4f})")
                    else:
                        st.write("ìƒìŠ¹í•œ ë…¸ë“œê°€ ì—†ìŠµë‹ˆë‹¤.")
                    
                    st.write("í•˜ë½ TOP 5:")
                    if len(ec_decreased) > 0:
                        for _, row in ec_decreased.iterrows():
                            st.write(f"ğŸ“‰ {row['Node']}")
                            st.write(f"   {row['EC_Prev']:.4f} â†’ {row['EC_Curr']:.4f} (ë³€í™”: {row['EC_Change']:.4f})")
                    else:
                        st.write("í•˜ë½í•œ ë…¸ë“œê°€ ì—†ìŠµë‹ˆë‹¤.")
                
                with col2:
                    st.write("**BC (ë§¤ê°œ ì¤‘ì‹¬ì„±) ë³€í™”**")
                    
                    bc_increased = centrality_df[centrality_df['BC_Change'] > 0].nlargest(5, 'BC_Change')[['Node', 'BC_Change', 'BC_Prev', 'BC_Curr']]
                    bc_decreased = centrality_df[centrality_df['BC_Change'] < 0].nsmallest(5, 'BC_Change')[['Node', 'BC_Change', 'BC_Prev', 'BC_Curr']]
                    
                    st.write("ìƒìŠ¹ TOP 5:")
                    if len(bc_increased) > 0:
                        for _, row in bc_increased.iterrows():
                            st.write(f"ğŸ“ˆ {row['Node']}")
                            st.write(f"   {row['BC_Prev']:.4f} â†’ {row['BC_Curr']:.4f} (ë³€í™”: +{row['BC_Change']:.4f})")
                    else:
                        st.write("ìƒìŠ¹í•œ ë…¸ë“œê°€ ì—†ìŠµë‹ˆë‹¤.")
                    
                    st.write("í•˜ë½ TOP 5:")
                    if len(bc_decreased) > 0:
                        for _, row in bc_decreased.iterrows():
                            st.write(f"ğŸ“‰ {row['Node']}")
                            st.write(f"   {row['BC_Prev']:.4f} â†’ {row['BC_Curr']:.4f} (ë³€í™”: {row['BC_Change']:.4f})")
                    else:
                        st.write("í•˜ë½í•œ ë…¸ë“œê°€ ì—†ìŠµë‹ˆë‹¤.")
                
                with col3:
                    st.write("**CC (ê·¼ì ‘ ì¤‘ì‹¬ì„±) ë³€í™”**")
                    
                    cc_increased = centrality_df[centrality_df['CC_Change'] > 0].nlargest(5, 'CC_Change')[['Node', 'CC_Change', 'CC_Prev', 'CC_Curr']]
                    cc_decreased = centrality_df[centrality_df['CC_Change'] < 0].nsmallest(5, 'CC_Change')[['Node', 'CC_Change', 'CC_Prev', 'CC_Curr']]
                    
                    st.write("ìƒìŠ¹ TOP 5:")
                    if len(cc_increased) > 0:
                        for _, row in cc_increased.iterrows():
                            st.write(f"ğŸ“ˆ {row['Node']}")
                            st.write(f"   {row['CC_Prev']:.4f} â†’ {row['CC_Curr']:.4f} (ë³€í™”: +{row['CC_Change']:.4f})")
                    else:
                        st.write("ìƒìŠ¹í•œ ë…¸ë“œê°€ ì—†ìŠµë‹ˆë‹¤.")
                    
                    st.write("í•˜ë½ TOP 5:")
                    if len(cc_decreased) > 0:
                        for _, row in cc_decreased.iterrows():
                            st.write(f"ğŸ“‰ {row['Node']}")
                            st.write(f"   {row['CC_Prev']:.4f} â†’ {row['CC_Curr']:.4f} (ë³€í™”: {row['CC_Change']:.4f})")
                    else:
                        st.write("í•˜ë½í•œ ë…¸ë“œê°€ ì—†ìŠµë‹ˆë‹¤.")
                
                # ì „ì²´ ë³€í™” ë°ì´í„° ë‹¤ìš´ë¡œë“œ
                with st.expander(f"{transition} ì „ì²´ ì¤‘ì‹¬ì„± ë³€í™” ë°ì´í„° ë³´ê¸°"):
                    st.dataframe(centrality_df.sort_values('EC_Change', ascending=False))
        
        st.divider()

def calculate_ipc_cooccurrence_timeseries(df, ipc_column, code_length=4, periods=None):
    """ì‹œê³„ì—´ì„ ê³ ë ¤í•œ ì¶•ì•½ëœ IPC ì½”ë“œ ê°„ì˜ ë™ì‹œì¶œí˜„ë¹ˆë„ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤."""

    # IPC ì»¬ëŸ¼ì´ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
    if ipc_column not in df.columns:
        st.error(f"'{ipc_column}' ì»¬ëŸ¼ì´ ë°ì´í„°í”„ë ˆì„ì— ì—†ìŠµë‹ˆë‹¤.")
        st.write("ì‚¬ìš© ê°€ëŠ¥í•œ ì»¬ëŸ¼:", df.columns.tolist())
        return {}, {}
    
    # ì „ì²´ ë°ì´í„°ì— ëŒ€í•œ ë¶„ì„
    results = {}
    
    # ì „ì²´ ê¸°ê°„ ë¶„ì„
    st.info("ì „ì²´ ê¸°ê°„ IPC ì½”ë“œ ë¶„ì„ ì¤‘...")
    edges_all, nodes_all = calculate_single_period_ipc(df, ipc_column, code_length, "ì „ì²´ ê¸°ê°„")
    results['ì „ì²´ ê¸°ê°„'] = {'edges': edges_all, 'nodes': nodes_all}
    
    # êµ¬ê°„ë³„ ë¶„ì„
    if periods:
        for period_name, period_info in periods.items():
            st.info(f"{period_name} IPC ì½”ë“œ ë¶„ì„ ì¤‘...")
            filtered_df = filter_data_by_period(df, 'ì¶œì›ì¼', period_info['start'], period_info['end'])
            
            if len(filtered_df) > 0:
                edges_period, nodes_period = calculate_single_period_ipc(filtered_df, ipc_column, code_length, period_name)
                results[period_name] = {'edges': edges_period, 'nodes': nodes_period}
            else:
                st.warning(f"{period_name}ì— í•´ë‹¹í•˜ëŠ” ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
                results[period_name] = {'edges': pd.DataFrame(), 'nodes': pd.DataFrame()}
    
    return results

def calculate_single_period_ipc(df, ipc_column, code_length, period_name):
    """ë‹¨ì¼ ê¸°ê°„ì— ëŒ€í•œ IPC ë™ì‹œì¶œí˜„ë¹ˆë„ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤."""
    all_combinations = []
    all_codes = []
    
    if len(df) == 0:
        # ë¹ˆ ë°ì´í„°í”„ë ˆì„ì¸ ê²½ìš° ë¹ˆ ê²°ê³¼ ë°˜í™˜
        empty_node_df = pd.DataFrame(columns=['id', 'Name', 'Label', 'EC', 'BC', 'CC', 'Degree', 'Weighted_Degree'])
        empty_edge_df = pd.DataFrame(columns=['Source', 'Target', 'type', 'Weight'])
        return empty_edge_df, empty_node_df
    
    progress_bar = st.progress(0)
    status_text = st.empty()
    
    total_rows = len(df)
    
    # ë°ì´í„°í”„ë ˆì„ì„ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜í•˜ì—¬ ì¸ë±ì‹± ë¬¸ì œ ë°©ì§€
    df_reset = df.reset_index(drop=True)
    
    for idx in range(total_rows):
        if idx % 10 == 0:
            progress = min(int((idx / total_rows) * 100), 100)
            progress_bar.progress(progress)
            status_text.text(f"{period_name} IPC ì½”ë“œ ì²˜ë¦¬ ì¤‘... {idx}/{total_rows} í–‰ ì™„ë£Œ ({progress}%)")
            
        row = df_reset.iloc[idx]
        ipc_str = row[ipc_column]
        ipc_codes = extract_shortened_ipc_codes(ipc_str, code_length)
        
        unique_codes = list(set(ipc_codes))
        all_codes.extend(unique_codes)
        
        if len(unique_codes) >= 2:
            pairs = list(combinations(unique_codes, 2))
            sorted_pairs = [tuple(sorted(pair)) for pair in pairs]
            all_combinations.extend(sorted_pairs)
    
    progress_bar.progress(100)
    status_text.text(f"{period_name} IPC ì½”ë“œ ì²˜ë¦¬ ì™„ë£Œ! ì´ {len(all_combinations)}ê°œì˜ ì¡°í•© ìƒì„±ë¨")
    
    # ë¹ˆë„ ê³„ì‚°
    counter = Counter(all_combinations)
    code_counter = Counter(all_codes)
    unique_codes = sorted(list(code_counter.keys()))
    
    # ë…¸ë“œ ë°ì´í„°í”„ë ˆì„ ìƒì„±
    node_df = pd.DataFrame({
        'id': range(len(unique_codes)),
        'Name': unique_codes,
        'Label': unique_codes
    })
    
    # ê¸°ë³¸ ì¤‘ì‹¬ì„± ì§€í‘œë¥¼ 0ìœ¼ë¡œ ì´ˆê¸°í™”
    node_df['EC'] = 0.0
    node_df['BC'] = 0.0
    node_df['CC'] = 0.0
    node_df['Degree'] = 0
    node_df['Weighted_Degree'] = 0.0
    
    # ì—£ì§€ ë°ì´í„° ìƒì„±
    if unique_codes and len(counter) > 0:
        ipc_to_id = dict(zip(unique_codes, node_df['id']))
        edges_data = []
        for (source, target), weight in counter.items():
            source_id = ipc_to_id[source]
            target_id = ipc_to_id[target]
            edges_data.append((source_id, target_id, 'undirected', weight))
        
        edge_df = pd.DataFrame(edges_data, columns=['Source', 'Target', 'type', 'Weight'])
        
        # ì¤‘ì‹¬ì„± ì§€í‘œ ê³„ì‚° (ì—£ì§€ê°€ ìˆëŠ” ê²½ìš°ì—ë§Œ)
        if len(edge_df) > 0:
            try:
                node_df_with_centrality = calculate_centrality_measures(edge_df, node_df)
                return edge_df, node_df_with_centrality
            except Exception as e:
                st.warning(f"ì¤‘ì‹¬ì„± ì§€í‘œ ê³„ì‚° ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
                return edge_df, node_df
        else:
            return edge_df, node_df
    else:
        edge_df = pd.DataFrame(columns=['Source', 'Target', 'type', 'Weight'])
        return edge_df, node_df

def calculate_entity_cooccurrence_timeseries(df, entity_column, entity_type="Entity", periods=None):
    """ì‹œê³„ì—´ì„ ê³ ë ¤í•œ ë°œëª…ìë‚˜ ì¶œì›ì¸ ë“±ì˜ ì—”í‹°í‹° ê°„ ë™ì‹œì¶œí˜„ë¹ˆë„ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤."""
    
    if entity_column not in df.columns:
        st.error(f"'{entity_column}' ì»¬ëŸ¼ì´ ë°ì´í„°í”„ë ˆì„ì— ì—†ìŠµë‹ˆë‹¤.")
        st.write("ì‚¬ìš© ê°€ëŠ¥í•œ ì»¬ëŸ¼:", df.columns.tolist())
        return {}
    
    results = {}
    
    # ì „ì²´ ê¸°ê°„ ë¶„ì„
    st.info(f"ì „ì²´ ê¸°ê°„ {entity_type} ë¶„ì„ ì¤‘...")
    edges_all, nodes_all = calculate_single_period_entity(df, entity_column, entity_type, "ì „ì²´ ê¸°ê°„")
    results['ì „ì²´ ê¸°ê°„'] = {'edges': edges_all, 'nodes': nodes_all}
    
    # êµ¬ê°„ë³„ ë¶„ì„
    if periods:
        for period_name, period_info in periods.items():
            st.info(f"{period_name} {entity_type} ë¶„ì„ ì¤‘...")
            filtered_df = filter_data_by_period(df, 'ì¶œì›ì¼', period_info['start'], period_info['end'])
            
            if len(filtered_df) > 0:
                edges_period, nodes_period = calculate_single_period_entity(filtered_df, entity_column, entity_type, period_name)
                results[period_name] = {'edges': edges_period, 'nodes': nodes_period}
            else:
                st.warning(f"{period_name}ì— í•´ë‹¹í•˜ëŠ” ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
                results[period_name] = {'edges': pd.DataFrame(), 'nodes': pd.DataFrame()}
    
    return results

def calculate_single_period_entity(df, entity_column, entity_type, period_name):
    """ë‹¨ì¼ ê¸°ê°„ì— ëŒ€í•œ ì—”í‹°í‹° ë™ì‹œì¶œí˜„ë¹ˆë„ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤."""
    all_combinations = []
    all_entities = []
    
    if len(df) == 0:
        # ë¹ˆ ë°ì´í„°í”„ë ˆì„ì¸ ê²½ìš° ë¹ˆ ê²°ê³¼ ë°˜í™˜
        empty_node_df = pd.DataFrame(columns=['id', 'Name', 'Label', 'EC', 'BC', 'CC', 'Degree', 'Weighted_Degree'])
        empty_edge_df = pd.DataFrame(columns=['Source', 'Target', 'type', 'Weight'])
        return empty_edge_df, empty_node_df
    
    progress_bar = st.progress(0)
    status_text = st.empty()
    
    total_rows = len(df)
    for idx, row in df.iterrows():
        if idx % 10 == 0:
            progress = min(int((idx / total_rows) * 100), 100)
            progress_bar.progress(progress)
            status_text.text(f"{period_name} {entity_type} ì²˜ë¦¬ ì¤‘... {idx}/{total_rows} í–‰ ì™„ë£Œ ({progress}%)")
            
        entity_str = row[entity_column]
        entities = extract_entities_from_delimited_string(entity_str, delimiter="|")
        
        unique_entities = list(set(entities))
        all_entities.extend(unique_entities)
        
        if len(unique_entities) >= 2:
            pairs = list(combinations(unique_entities, 2))
            sorted_pairs = [tuple(sorted(pair)) for pair in pairs]
            all_combinations.extend(sorted_pairs)
    
    progress_bar.progress(100)
    status_text.text(f"{period_name} {entity_type} ì²˜ë¦¬ ì™„ë£Œ! ì´ {len(all_combinations)}ê°œì˜ ì¡°í•© ìƒì„±ë¨")
    
    # ë¹ˆë„ ê³„ì‚°
    counter = Counter(all_combinations)
    entity_counter = Counter(all_entities)
    unique_entities = sorted(list(entity_counter.keys()))
    
    # ë…¸ë“œ ë°ì´í„°í”„ë ˆì„ ìƒì„±
    node_df = pd.DataFrame({
        'id': range(len(unique_entities)),
        'Name': unique_entities,
        'Label': unique_entities
    })
    
    # ê¸°ë³¸ ì¤‘ì‹¬ì„± ì§€í‘œë¥¼ 0ìœ¼ë¡œ ì´ˆê¸°í™”
    node_df['EC'] = 0.0
    node_df['BC'] = 0.0
    node_df['CC'] = 0.0
    node_df['Degree'] = 0
    node_df['Weighted_Degree'] = 0.0
    
    # ì—£ì§€ ë°ì´í„° ìƒì„±
    if unique_entities and len(counter) > 0:
        entity_to_id = dict(zip(unique_entities, node_df['id']))
        edges_data = []
        for (source, target), weight in counter.items():
            source_id = entity_to_id[source]
            target_id = entity_to_id[target]
            edges_data.append((source_id, target_id, 'undirected', weight))
        
        edge_df = pd.DataFrame(edges_data, columns=['Source', 'Target', 'type', 'Weight'])
        
        # ì¤‘ì‹¬ì„± ì§€í‘œ ê³„ì‚° (ì—£ì§€ê°€ ìˆëŠ” ê²½ìš°ì—ë§Œ)
        if len(edge_df) > 0:
            try:
                node_df_with_centrality = calculate_centrality_measures(edge_df, node_df)
                return edge_df, node_df_with_centrality
            except Exception as e:
                st.warning(f"ì¤‘ì‹¬ì„± ì§€í‘œ ê³„ì‚° ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
                return edge_df, node_df
        else:
            return edge_df, node_df
    else:
        edge_df = pd.DataFrame(columns=['Source', 'Target', 'type', 'Weight'])
        return edge_df, node_df

def apply_label_mapping(node_df, mapping_file, code_length=4):
    """ë§¤í•‘ í…Œì´ë¸”ì„ ì‚¬ìš©í•˜ì—¬ ë…¸ë“œ ë°ì´í„°ì˜ Label ì»¬ëŸ¼ì„ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤."""
    try:
        try:
            mapping_df = pd.read_excel(mapping_file)
        except Exception as e:
            st.error(f"ë§¤í•‘ í…Œì´ë¸” ë¡œë“œ ì˜¤ë¥˜: {e}")
            return node_df
        
        st.write("ë§¤í•‘ í…Œì´ë¸” ì»¬ëŸ¼:", mapping_df.columns.tolist())
        
        ipc_column_candidates = ['IPC', 'Code', 'ipc', 'code', 'Symbol', 'symbol', 'Name']
        label_column_candidates = ['Label', 'Description', 'label', 'description', 'Title', 'title', 'Definition', 'definition']
        
        ipc_column_name = None
        label_column_name = None
        
        for col in mapping_df.columns:
            if any(cand.lower() in col.lower() for cand in ipc_column_candidates):
                sample_values = mapping_df[col].dropna().astype(str).head(3).tolist()
                
                if code_length == 4 and any(re.match(r'^[A-Z]\d{2}[A-Z]', str(val)) for val in sample_values):
                    ipc_column_name = col
                elif code_length == 8 and any(re.match(r'^[A-Z]\d{2}[A-Z]-\d{3}', str(val)) for val in sample_values):
                    ipc_column_name = col
            
            if any(cand.lower() in col.lower() for cand in label_column_candidates):
                label_column_name = col
        
        if not ipc_column_name or not label_column_name:
            st.warning("ë§¤í•‘ í…Œì´ë¸”ì—ì„œ ì»¬ëŸ¼ì„ ìë™ìœ¼ë¡œ ì‹ë³„í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            
            ipc_column_name = st.selectbox(
                "IPC ì½”ë“œê°€ ìˆëŠ” ì»¬ëŸ¼ì„ ì„ íƒí•˜ì„¸ìš”:", 
                options=mapping_df.columns, 
                index=0 if mapping_df.columns.any() else None
            )
            
            label_column_name = st.selectbox(
                "ë ˆì´ë¸” ì •ë³´ê°€ ìˆëŠ” ì»¬ëŸ¼ì„ ì„ íƒí•˜ì„¸ìš”:", 
                options=[col for col in mapping_df.columns if col != ipc_column_name], 
                index=0 if len(mapping_df.columns) > 1 else None
            )
        
        if ipc_column_name and label_column_name:
            mapping_df[ipc_column_name] = mapping_df[ipc_column_name].astype(str)
            
            if mapping_df[ipc_column_name].duplicated().any():
                st.warning("ë§¤í•‘ í…Œì´ë¸”ì— ì¤‘ë³µëœ IPC ì½”ë“œê°€ ìˆìŠµë‹ˆë‹¤. ê° ì½”ë“œì˜ ì²« ë²ˆì§¸ í•­ëª©ë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤.")
                mapping_df = mapping_df.drop_duplicates(subset=[ipc_column_name], keep='first')
            
            ipc_to_label = dict(zip(mapping_df[ipc_column_name], mapping_df[label_column_name]))
            
            node_df['OriginalLabel'] = node_df['Label']
            node_df['Label'] = node_df['Name'].map(ipc_to_label)
            node_df.loc[node_df['Label'].isna(), 'Label'] = node_df.loc[node_df['Label'].isna(), 'Name']
            
            mapped_count = (node_df['Label'] != node_df['OriginalLabel']).sum()
            st.success(f"ì´ {len(node_df)}ê°œ ë…¸ë“œ ì¤‘ {mapped_count}ê°œ ë…¸ë“œì˜ ë ˆì´ë¸”ì´ ì—…ë°ì´íŠ¸ë˜ì—ˆìŠµë‹ˆë‹¤.")
            
            node_df = node_df.drop(columns=['OriginalLabel'])
            
            return node_df
        else:
            st.warning("í•„ìš”í•œ ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ì–´ ë ˆì´ë¸” ë§¤í•‘ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
            return node_df
    
    except Exception as e:
        st.error(f"ë ˆì´ë¸” ë§¤í•‘ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return node_df

def create_zip_file(files_dict):
    """ì—¬ëŸ¬ íŒŒì¼ì„ ì••ì¶•í•˜ì—¬ ë‹¤ìš´ë¡œë“œ ê°€ëŠ¥í•œ ë§í¬ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
    with tempfile.TemporaryDirectory() as temp_dir:
        zip_path = os.path.join(temp_dir, "patent_network_files.zip")
        
        with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:
            for filename, content in files_dict.items():
                content_bytes = content.encode('utf-8')
                zipf.writestr(filename, content_bytes)
        
        with open(zip_path, 'rb') as f:
            bytes_data = f.read()
        
        b64 = base64.b64encode(bytes_data).decode()
        return f'<a href="data:application/zip;base64,{b64}" download="patent_network_files.zip">ë‹¤ìš´ë¡œë“œ: íŠ¹í—ˆ ë„¤íŠ¸ì›Œí¬ ë¶„ì„ íŒŒì¼</a>'

def get_download_link(df, filename):
    """ë°ì´í„°í”„ë ˆì„ì„ CSV íŒŒì¼ë¡œ ë³€í™˜í•˜ì—¬ ë‹¤ìš´ë¡œë“œ ë§í¬ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
    csv = df.to_csv(index=False)
    b64 = base64.b64encode(csv.encode('utf-8-sig')).decode()
    href = f'<a href="data:file/csv;base64,{b64}" download="{filename}">ë‹¤ìš´ë¡œë“œ: {filename}</a>'
    return href

def get_readme_content(results_dict, periods_info=None):
    """README íŒŒì¼ ë‚´ìš©ì„ ìƒì„±í•©ë‹ˆë‹¤."""
    readme = """# íŠ¹í—ˆ ì‹œê³„ì—´ ë„¤íŠ¸ì›Œí¬ ë¶„ì„ íŒŒì¼ ì„¤ëª…

## ì¤‘ì‹¬ì„± ì§€í‘œ ì„¤ëª…
- **EC (Eigenvector Centrality)**: ê³ ìœ ë²¡í„° ì¤‘ì‹¬ì„± - ì¤‘ìš”í•œ ë…¸ë“œë“¤ê³¼ ì—°ê²°ëœ ë…¸ë“œì˜ ì¤‘ìš”ë„
- **BC (Betweenness Centrality)**: ë§¤ê°œ ì¤‘ì‹¬ì„± - ë‹¤ë¥¸ ë…¸ë“œë“¤ ì‚¬ì´ì˜ ìµœë‹¨ ê²½ë¡œì— ìœ„ì¹˜í•˜ëŠ” ì •ë„
- **CC (Closeness Centrality)**: ê·¼ì ‘ ì¤‘ì‹¬ì„± - ë‹¤ë¥¸ ëª¨ë“  ë…¸ë“œë“¤ê³¼ì˜ í‰ê·  ê±°ë¦¬ì˜ ì—­ìˆ˜
- **Degree**: ì—°ê²° ìˆ˜ - ì§ì ‘ ì—°ê²°ëœ ë…¸ë“œì˜ ê°œìˆ˜
- **Weighted_Degree**: ê°€ì¤‘ ì—°ê²° ìˆ˜ - ì—°ê²° ê°•ë„ë¥¼ ê³ ë ¤í•œ ì—°ê²° ìˆ˜

## ì‹œê³„ì—´ ë¶„ì„ ì •ë³´
"""
    
    if periods_info:
        readme += "### ë¶„ì„ êµ¬ê°„\n"
        for period_name, period_info in periods_info.items():
            readme += f"- **{period_name}**: {period_info['start']}ë…„ ~ {period_info['end']}ë…„\n"
        readme += "\n"
    
    readme += """
## íŒŒì¼ êµ¬ì¡°
ê° ë¶„ì„ í•­ëª©(IPC ì½”ë“œ, ë°œëª…ì, ì¶œì›ì¸)ì— ëŒ€í•´ ë‹¤ìŒê³¼ ê°™ì€ íŒŒì¼ë“¤ì´ ìƒì„±ë©ë‹ˆë‹¤:
- ì „ì²´ ê¸°ê°„ ë¶„ì„ íŒŒì¼
- êµ¬ê°„ë³„ ë¶„ì„ íŒŒì¼ (ì„¤ì •í•œ êµ¬ê°„ì´ ìˆëŠ” ê²½ìš°)

## Gephi ì‚¬ìš© ë°©ë²•
1. Gephië¥¼ ì‹¤í–‰í•˜ê³  ìƒˆ í”„ë¡œì íŠ¸ ìƒì„±
2. ë°ì´í„° ì—°êµ¬ì‹¤(Data Laboratory) íƒ­ ì„ íƒ
3. 'ë…¸ë“œ í…Œì´ë¸” ê°€ì ¸ì˜¤ê¸°' í´ë¦­ í›„ *_nodes.csv íŒŒì¼ ë¶ˆëŸ¬ì˜¤ê¸°
4. 'ì—£ì§€ í…Œì´ë¸” ê°€ì ¸ì˜¤ê¸°' í´ë¦­ í›„ *_edges.csv íŒŒì¼ ë¶ˆëŸ¬ì˜¤ê¸°
5. 'ê°œìš”(Overview)' íƒ­ìœ¼ë¡œ ì´ë™í•˜ì—¬ ë„¤íŠ¸ì›Œí¬ ì‹œê°í™” ë° ë¶„ì„
6. ë…¸ë“œ í¬ê¸°ë‚˜ ìƒ‰ìƒì„ ì¤‘ì‹¬ì„± ì§€í‘œì— ë”°ë¼ ì¡°ì •í•˜ì—¬ ì‹œê°í™”
7. ì‹œê³„ì—´ ë¹„êµë¥¼ ìœ„í•´ ì—¬ëŸ¬ ì‹œê¸°ì˜ íŒŒì¼ì„ ë³„ë„ë¡œ ë¶„ì„
"""
    return readme

def display_top_pairs(edges_df, nodes_df, pair_type="ìŒ"):
    """ìƒìœ„ ë™ì‹œì¶œí˜„ ìŒì„ í‘œì‹œí•˜ëŠ” í•¨ìˆ˜"""
    if len(edges_df) == 0:
        st.warning(f"ë¶„ì„í•  {pair_type} ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    st.subheader(f"ë™ì‹œì¶œí˜„ë¹ˆë„ ìƒìœ„ 10ê°œ {pair_type}")
    
    top_edges = edges_df.sort_values(by='Weight', ascending=False).head(10)
    
    for idx, row in top_edges.iterrows():
        source_name = nodes_df.loc[nodes_df['id'] == row['Source'], 'Name'].values[0]
        target_name = nodes_df.loc[nodes_df['id'] == row['Target'], 'Name'].values[0]
        source_label = nodes_df.loc[nodes_df['id'] == row['Source'], 'Label'].values[0]
        target_label = nodes_df.loc[nodes_df['id'] == row['Target'], 'Label'].values[0]
        
        st.write(f"**{source_name}** ({source_label}) â†” **{target_name}** ({target_label}): {row['Weight']}íšŒ")

def display_top_centrality_nodes(nodes_df, centrality_column, pair_type="ë…¸ë“œ", top_n=10):
    """ì¤‘ì‹¬ì„± ì§€í‘œ ìƒìœ„ ë…¸ë“œë“¤ì„ í‘œì‹œí•˜ëŠ” í•¨ìˆ˜"""
    if centrality_column not in nodes_df.columns:
        st.warning(f"{centrality_column} ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    st.subheader(f"{centrality_column} ìƒìœ„ {top_n}ê°œ {pair_type}")
    
    top_nodes = nodes_df.sort_values(by=centrality_column, ascending=False).head(top_n)
    
    display_df = top_nodes[['Name', 'Label', centrality_column]].copy()
    display_df.index = range(1, len(display_df) + 1)
    st.dataframe(display_df)

def display_period_comparison(results_dict, analysis_type):
    """êµ¬ê°„ë³„ ë¹„êµ ë¶„ì„ì„ í‘œì‹œí•˜ëŠ” í•¨ìˆ˜"""
    if len(results_dict) <= 1:
        return
    
    st.subheader(f"{analysis_type} êµ¬ê°„ë³„ ë¹„êµ")
    
    # êµ¬ê°„ë³„ í†µê³„ ìˆ˜ì§‘
    comparison_data = []
    for period_name, result in results_dict.items():
        nodes_df = result['nodes']
        edges_df = result['edges']
        
        if len(nodes_df) > 0:
            avg_ec = nodes_df['EC'].mean() if 'EC' in nodes_df.columns else 0
            avg_bc = nodes_df['BC'].mean() if 'BC' in nodes_df.columns else 0
            avg_cc = nodes_df['CC'].mean() if 'CC' in nodes_df.columns else 0
            
            comparison_data.append({
                'êµ¬ê°„': period_name,
                'ë…¸ë“œ ìˆ˜': len(nodes_df),
                'ì—£ì§€ ìˆ˜': len(edges_df),
                'í‰ê·  EC': round(avg_ec, 4),
                'í‰ê·  BC': round(avg_bc, 4),
                'í‰ê·  CC': round(avg_cc, 4),
                'ìµœëŒ€ ê°€ì¤‘ì¹˜': edges_df['Weight'].max() if len(edges_df) > 0 else 0
            })
    
    if comparison_data:
        comparison_df = pd.DataFrame(comparison_data)
        st.dataframe(comparison_df)

def identify_monotonic_centrality_nodes(yearly_results, metric='EC', min_length=3):
    from collections import defaultdict

    node_trends = defaultdict(list)
    sorted_years = sorted(yearly_results.keys())

    for year in sorted_years:
        df = yearly_results[year]
        for _, row in df.iterrows():
            node = row['Name']
            value = row.get(metric, None)
            if value is not None:
                node_trends[node].append(value)

    increasing = []
    decreasing = []

    for node, values in node_trends.items():
        if len(values) >= min_length:
            if all(earlier <= later for earlier, later in zip(values, values[1:])):
                increasing.append((node, values))
            elif all(earlier >= later for earlier, later in zip(values, values[1:])):
                decreasing.append((node, values))

    return increasing, decreasing

# ë©”ì¸ ì• í”Œë¦¬ì¼€ì´ì…˜ ë¡œì§
def main():
    # í•œê¸€ í°íŠ¸ ì„¤ì •
    setup_korean_font()
    
    # ì‚¬ì´ë“œë°” ì„¤ì •
    st.sidebar.header("ì„¤ì •")
    
    # íŒŒì¼ ì—…ë¡œë“œ ì„¹ì…˜
    st.sidebar.subheader("1. ë°ì´í„° íŒŒì¼ ì—…ë¡œë“œ")
    uploaded_file = st.sidebar.file_uploader(
        "Excel íŠ¹í—ˆ ë°ì´í„° íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”", 
        type=["xlsx", "xls"],
        key="main_patent_data_file"  # ê³ ìœ í•œ í‚¤ ì¶”ê°€
    )
    
    # ë¶„ì„ ì˜µì…˜ ì„ íƒ
    st.sidebar.subheader("2. ë¶„ì„ ì˜µì…˜ ì„ íƒ")
    analyze_ipc = st.sidebar.checkbox("IPC ì½”ë“œ ë¶„ì„", value=True)
    analyze_inventor = st.sidebar.checkbox("ë°œëª…ì ë¶„ì„", value=True)
    analyze_applicant = st.sidebar.checkbox("ì¶œì›ì¸ ë¶„ì„", value=True)
    
    # ì¤‘ì‹¬ì„± ë¶„ì„ ì˜µì…˜
    st.sidebar.subheader("3. ì¤‘ì‹¬ì„± ë¶„ì„ ì˜µì…˜")
    calculate_centrality = st.sidebar.checkbox("ì¤‘ì‹¬ì„± ì§€í‘œ ê³„ì‚° (EC, BC, CC)", value=True)
    
    # ì‹œê³„ì—´ ë¶„ì„ ì˜µì…˜
    st.sidebar.subheader("4. ì‹œê³„ì—´ ë¶„ì„ ì„¤ì •")
    enable_timeseries = st.sidebar.checkbox("ì‹œê³„ì—´ ë¶„ì„ í™œì„±í™”", value=True)
    
    periods = {}
    if enable_timeseries:
        num_periods = st.sidebar.selectbox("ë¹„êµ êµ¬ê°„ ìˆ˜", [0, 1, 2, 3], index=0)
        
        if num_periods > 0:
            st.sidebar.write("**êµ¬ê°„ ì„¤ì •**")
            for i in range(num_periods):
                period_name = st.sidebar.text_input(f"êµ¬ê°„ {i+1} ì´ë¦„", value=f"êµ¬ê°„ {i+1}", key=f"period_name_{i}")
                
                col1, col2 = st.sidebar.columns(2)
                with col1:
                    start_year = st.number_input(f"ì‹œì‘ë…„ë„", min_value=1990, max_value=2030, value=2000+i*5, key=f"start_year_{i}")
                with col2:
                    end_year = st.number_input(f"ì¢…ë£Œë…„ë„", min_value=1990, max_value=2030, value=2005+i*5, key=f"end_year_{i}")
                
                if start_year <= end_year:
                    periods[period_name] = {'start': start_year, 'end': end_year}
                else:
                    st.sidebar.error(f"{period_name}: ì‹œì‘ë…„ë„ê°€ ì¢…ë£Œë…„ë„ë³´ë‹¤ í´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    
    # ë„¤íŠ¸ì›Œí¬ ì‹œê°í™” ì˜µì…˜
    st.sidebar.subheader("5. ğŸ†• ë„¤íŠ¸ì›Œí¬ ì‹œê°í™” ì„¤ì •")
    enable_network_viz = st.sidebar.checkbox("ë„¤íŠ¸ì›Œí¬ ì‹œê°í™” í™œì„±í™”", value=True)
    
    if enable_network_viz:
        max_nodes = st.sidebar.slider("ìµœëŒ€ ë…¸ë“œ ìˆ˜", min_value=20, max_value=200, value=100, step=10)
        min_edge_weight = st.sidebar.slider("ìµœì†Œ ì—£ì§€ ê°€ì¤‘ì¹˜", min_value=1, max_value=10, value=1, step=1)
        
        node_size_options = ['Degree', 'Weighted_Degree', 'EC', 'BC', 'CC']
        node_size_metric = st.sidebar.selectbox("ë…¸ë“œ í¬ê¸° ê¸°ì¤€", node_size_options, index=0)
        
        node_color_options = ['EC', 'BC', 'CC', 'Degree', 'Community']
        node_color_metric = st.sidebar.selectbox("ë…¸ë“œ ìƒ‰ìƒ ê¸°ì¤€", node_color_options, index=0)
    
    # ë§¤í•‘ íŒŒì¼ ì—…ë¡œë“œ ì„¹ì…˜
    st.sidebar.subheader("6. ë ˆì´ë¸” ë§¤í•‘ íŒŒì¼ (ì„ íƒì‚¬í•­)")
    uploaded_mapping_4chars = st.sidebar.file_uploader(
        "4ìë¦¬ IPC ì½”ë“œ ë§¤í•‘ íŒŒì¼ (ì„ íƒì‚¬í•­)", 
        type=["xlsx", "xls"], 
        key="mapping_4chars_file"
    )
    uploaded_mapping_8chars = st.sidebar.file_uploader(
        "8ìë¦¬ IPC ì½”ë“œ ë§¤í•‘ íŒŒì¼ (ì„ íƒì‚¬í•­)", 
        type=["xlsx", "xls"], 
        key="mapping_8chars_file"
    )
    
    # ì²˜ë¦¬ ë²„íŠ¼
    process_button = st.sidebar.button("ë¶„ì„ ì‹œì‘", type="primary")
    
    # ë©”ì¸ í™”ë©´ ë ˆì´ì•„ì›ƒ - íƒ­ êµ¬ì„±
    tab_names = ["ë¶„ì„ ê²°ê³¼", "ì‹œê³„ì—´ ëŒ€ì‹œë³´ë“œ"]
    
    if enable_network_viz:
        tab_names.append("ğŸ†• ë„¤íŠ¸ì›Œí¬ ì‹œê°í™”")
    
    if analyze_ipc:
        tab_names.extend(["4ìë¦¬ IPC ì½”ë“œ", "8ìë¦¬ IPC ì½”ë“œ"])
    if analyze_inventor:
        tab_names.append("ë°œëª…ì ë¶„ì„")
    if analyze_applicant:
        tab_names.append("ì¶œì›ì¸ ë¶„ì„")
    
    # ìƒˆë¡œìš´ ì‹œê³„ì—´ ì¤‘ì‹¬ì„± ë¶„ì„ íƒ­ ì¶”ê°€
    if enable_timeseries:
        tab_names.append("ì¤‘ì‹¬ì„± ì¶”ì´ ë¶„ì„")
        tab_names.append("êµ¬ê°„ë³„ ë³€í™” ë¶„ì„")
    
    # íƒ­ ìƒì„± ë° ë”•ì…”ë„ˆë¦¬ì— ì €ì¥
    tabs = st.tabs(tab_names)
    tabs_dict = {}
    for i, tab_name in enumerate(tab_names):
        tabs_dict[tab_name] = tabs[i]
    
    if uploaded_file is not None:
        # íŒŒì¼ ë¡œë“œ ë° ì²˜ë¦¬
        with st.spinner('ë°ì´í„° íŒŒì¼ ë¡œë“œ ì¤‘...'):
            df = pd.read_excel(uploaded_file)
            st.success(f"Excel íŒŒì¼ì„ ì„±ê³µì ìœ¼ë¡œ ë¡œë“œí–ˆìŠµë‹ˆë‹¤. í–‰ ìˆ˜: {len(df)}")
            
            # ì¶œì›ì¼ ì»¬ëŸ¼ í™•ì¸
            date_columns = [col for col in df.columns if 'ì¶œì›ì¼' in col or 'date' in col.lower() or 'ì¼ì' in col]
            
            # ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°
            with tabs_dict["ë¶„ì„ ê²°ê³¼"]:
                st.subheader("ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°")
                st.dataframe(df.head(5))
                
                # ì¶œì›ì¼ ì»¬ëŸ¼ ì„ íƒ
                if enable_timeseries:
                    st.subheader("ì¶œì›ì¼ ì»¬ëŸ¼ ì„ íƒ")
                    if date_columns:
                        default_date_index = 0
                    else:
                        default_date_index = 0
                        st.warning("ì¶œì›ì¼ ê´€ë ¨ ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ìˆ˜ë™ìœ¼ë¡œ ì„ íƒí•´ì£¼ì„¸ìš”.")
                    
                    selected_date_column = st.selectbox(
                        "ì¶œì›ì¼ì´ í¬í•¨ëœ ì»¬ëŸ¼ì„ ì„ íƒí•˜ì„¸ìš”:",
                        options=df.columns.tolist(),
                        index=default_date_index,
                        key="date_column"
                    )
                
                # ì»¬ëŸ¼ ì„ íƒ
                st.subheader("ë¶„ì„ ì»¬ëŸ¼ ì„ íƒ")
                
                selected_columns = {}
                
                if analyze_ipc:
                    ipc_column_candidates = [col for col in df.columns if 'ipc' in col.lower() or 'code' in col.lower()]
                    default_index = 0
                    
                    if ipc_column_candidates:
                        try:
                            first_candidate = ipc_column_candidates[0]
                            if first_candidate in df.columns:
                                default_index = int(df.columns.get_loc(first_candidate))
                            else:
                                default_index = 0
                        except:
                            default_index = 0
                    
                    selected_columns['ipc'] = st.selectbox(
                        "IPC ì½”ë“œê°€ í¬í•¨ëœ ì»¬ëŸ¼ì„ ì„ íƒí•˜ì„¸ìš”:",
                        options=df.columns.tolist(),
                        index=default_index,
                        key="ipc_column"
                    )
                
                if analyze_inventor:
                    inventor_column_candidates = [col for col in df.columns if 'ë°œëª…ì' in col or 'inventor' in col.lower() or 'ê°œë°œì' in col]
                    default_index = 0
                    
                    if inventor_column_candidates:
                        try:
                            first_candidate = inventor_column_candidates[0]
                            if first_candidate in df.columns:
                                default_index = int(df.columns.get_loc(first_candidate))
                            else:
                                default_index = 0
                        except:
                            default_index = 0
                    
                    selected_columns['inventor'] = st.selectbox(
                        "ë°œëª…ìê°€ í¬í•¨ëœ ì»¬ëŸ¼ì„ ì„ íƒí•˜ì„¸ìš”:",
                        options=df.columns.tolist(),
                        index=default_index,
                        key="inventor_column"
                    )
                
                if analyze_applicant:
                    applicant_column_candidates = [col for col in df.columns if 'ì¶œì›ì¸' in col or 'applicant' in col.lower() or 'ì§€ì›ì' in col]
                    default_index = 0
                    
                    if applicant_column_candidates:
                        try:
                            first_candidate = applicant_column_candidates[0]
                            if first_candidate in df.columns:
                                default_index = int(df.columns.get_loc(first_candidate))
                            else:
                                default_index = 0
                        except:
                            default_index = 0
                    
                    selected_columns['applicant'] = st.selectbox(
                        "ì¶œì›ì¸ì´ í¬í•¨ëœ ì»¬ëŸ¼ì„ ì„ íƒí•˜ì„¸ìš”:",
                        options=df.columns.tolist(),
                        index=default_index,
                        key="applicant_column"
                    )
        
        if process_button:
            # ì¶œì›ì¼ ì»¬ëŸ¼ ì„¤ì •
            if enable_timeseries:
                df['ì¶œì›ì¼'] = df[selected_date_column]
            
            # ë¶„ì„ ê²°ê³¼ë¥¼ ì €ì¥í•  ë”•ì…”ë„ˆë¦¬
            results = {}
            yearly_centrality_results = {}
            
            with tabs_dict["ë¶„ì„ ê²°ê³¼"]:
                st.header("ë¶„ì„ ì§„í–‰ ì¤‘...")
                
                # IPC ì½”ë“œ ë¶„ì„
                if analyze_ipc:
                    st.subheader("IPC ì½”ë“œ ì‹œê³„ì—´ ë¶„ì„ ì¤‘...")
                    
                    # 4ìë¦¬ IPC ì½”ë“œ
                    if enable_timeseries:
                        results['ipc_4'] = calculate_ipc_cooccurrence_timeseries(df, selected_columns['ipc'], code_length=4, periods=periods)
                        yearly_centrality_results['ipc_4'] = calculate_yearly_centrality_analysis(df, selected_columns['ipc'], "IPC", code_length=4)
                    else:
                        edges_4, nodes_4 = calculate_single_period_ipc(df, selected_columns['ipc'], 4, "ì „ì²´ ê¸°ê°„")
                        results['ipc_4'] = {'ì „ì²´ ê¸°ê°„': {'edges': edges_4, 'nodes': nodes_4}}
                    
                    # 8ìë¦¬ IPC ì½”ë“œ
                    if enable_timeseries:
                        results['ipc_8'] = calculate_ipc_cooccurrence_timeseries(df, selected_columns['ipc'], code_length=8, periods=periods)
                        yearly_centrality_results['ipc_8'] = calculate_yearly_centrality_analysis(df, selected_columns['ipc'], "IPC", code_length=8)
                    else:
                        edges_8, nodes_8 = calculate_single_period_ipc(df, selected_columns['ipc'], 8, "ì „ì²´ ê¸°ê°„")
                        results['ipc_8'] = {'ì „ì²´ ê¸°ê°„': {'edges': edges_8, 'nodes': nodes_8}}
                    
                    # ë ˆì´ë¸” ë§¤í•‘ ì ìš©
                    if uploaded_mapping_4chars is not None:
                        st.subheader("4ìë¦¬ IPC ì½”ë“œì— ë§¤í•‘ í…Œì´ë¸” ì ìš© ì¤‘...")
                        for period_name in results['ipc_4'].keys():
                            results['ipc_4'][period_name]['nodes'] = apply_label_mapping(
                                results['ipc_4'][period_name]['nodes'], uploaded_mapping_4chars, code_length=4)
                    
                    if uploaded_mapping_8chars is not None:
                        st.subheader("8ìë¦¬ IPC ì½”ë“œì— ë§¤í•‘ í…Œì´ë¸” ì ìš© ì¤‘...")
                        for period_name in results['ipc_8'].keys():
                            results['ipc_8'][period_name]['nodes'] = apply_label_mapping(
                                results['ipc_8'][period_name]['nodes'], uploaded_mapping_8chars, code_length=8)
                
                # ë°œëª…ì ë¶„ì„
                if analyze_inventor:
                    st.subheader("ë°œëª…ì ì‹œê³„ì—´ ë¶„ì„ ì¤‘...")
                    if enable_timeseries:
                        results['inventor'] = calculate_entity_cooccurrence_timeseries(df, selected_columns['inventor'], "ë°œëª…ì", periods=periods)
                        yearly_centrality_results['inventor'] = calculate_yearly_centrality_analysis(df, selected_columns['inventor'], "ë°œëª…ì")
                    else:
                        edges_inv, nodes_inv = calculate_single_period_entity(df, selected_columns['inventor'], "ë°œëª…ì", "ì „ì²´ ê¸°ê°„")
                        results['inventor'] = {'ì „ì²´ ê¸°ê°„': {'edges': edges_inv, 'nodes': nodes_inv}}
                
                # ì¶œì›ì¸ ë¶„ì„
                if analyze_applicant:
                    st.subheader("ì¶œì›ì¸ ì‹œê³„ì—´ ë¶„ì„ ì¤‘...")
                    if enable_timeseries:
                        results['applicant'] = calculate_entity_cooccurrence_timeseries(df, selected_columns['applicant'], "ì¶œì›ì¸", periods=periods)
                        yearly_centrality_results['applicant'] = calculate_yearly_centrality_analysis(df, selected_columns['applicant'], "ì¶œì›ì¸")
                    else:
                        edges_app, nodes_app = calculate_single_period_entity(df, selected_columns['applicant'], "ì¶œì›ì¸", "ì „ì²´ ê¸°ê°„")
                        results['applicant'] = {'ì „ì²´ ê¸°ê°„': {'edges': edges_app, 'nodes': nodes_app}}
                
                # ë¶„ì„ ê²°ê³¼ ìš”ì•½ í‘œì‹œ
                st.header("ë¶„ì„ ì™„ë£Œ!")
                
                summary_data = []
                if analyze_ipc:
                    for period_name, result in results['ipc_4'].items():
                        summary_data.append({
                            'ë¶„ì„ í•­ëª©': '4ìë¦¬ IPC ì½”ë“œ',
                            'êµ¬ê°„': period_name,
                            'ë…¸ë“œ ìˆ˜': len(result['nodes']),
                            'ì—£ì§€ ìˆ˜': len(result['edges'])
                        })
                    
                    for period_name, result in results['ipc_8'].items():
                        summary_data.append({
                            'ë¶„ì„ í•­ëª©': '8ìë¦¬ IPC ì½”ë“œ',
                            'êµ¬ê°„': period_name,
                            'ë…¸ë“œ ìˆ˜': len(result['nodes']),
                            'ì—£ì§€ ìˆ˜': len(result['edges'])
                        })
                
                if analyze_inventor:
                    for period_name, result in results['inventor'].items():
                        summary_data.append({
                            'ë¶„ì„ í•­ëª©': 'ë°œëª…ì',
                            'êµ¬ê°„': period_name,
                            'ë…¸ë“œ ìˆ˜': len(result['nodes']),
                            'ì—£ì§€ ìˆ˜': len(result['edges'])
                        })
                
                if analyze_applicant:
                    for period_name, result in results['applicant'].items():
                        summary_data.append({
                            'ë¶„ì„ í•­ëª©': 'ì¶œì›ì¸',
                            'êµ¬ê°„': period_name,
                            'ë…¸ë“œ ìˆ˜': len(result['nodes']),
                            'ì—£ì§€ ìˆ˜': len(result['edges'])
                        })
                
                if summary_data:
                    summary_df = pd.DataFrame(summary_data)
                    st.dataframe(summary_df)
                
                # ê²°ê³¼ íŒŒì¼ ìƒì„±
                files_dict = {}
                
                # ê° ë¶„ì„ ìœ í˜•ë³„ë¡œ íŒŒì¼ ìƒì„±
                for analysis_type, analysis_results in results.items():
                    for period_name, result in analysis_results.items():
                        period_suffix = f"_{period_name.replace(' ', '_')}" if period_name != "ì „ì²´ ê¸°ê°„" else ""
                        
                        if analysis_type == 'ipc_4':
                            files_dict[f"ipc_edges_4chars{period_suffix}.csv"] = result['edges'].to_csv(index=False)
                            files_dict[f"ipc_nodes_4chars{period_suffix}.csv"] = result['nodes'].to_csv(index=False)
                        elif analysis_type == 'ipc_8':
                            files_dict[f"ipc_edges_8chars{period_suffix}.csv"] = result['edges'].to_csv(index=False)
                            files_dict[f"ipc_nodes_8chars{period_suffix}.csv"] = result['nodes'].to_csv(index=False)
                        elif analysis_type == 'inventor':
                            files_dict[f"inventor_edges{period_suffix}.csv"] = result['edges'].to_csv(index=False)
                            files_dict[f"inventor_nodes{period_suffix}.csv"] = result['nodes'].to_csv(index=False)
                        elif analysis_type == 'applicant':
                            files_dict[f"applicant_edges{period_suffix}.csv"] = result['edges'].to_csv(index=False)
                            files_dict[f"applicant_nodes{period_suffix}.csv"] = result['nodes'].to_csv(index=False)
                
                files_dict["README.txt"] = get_readme_content(results, periods)
                
                # ë‹¤ìš´ë¡œë“œ ë§í¬ ìƒì„±
                download_link = create_zip_file(files_dict)
                st.markdown(download_link, unsafe_allow_html=True)
            
            # ğŸ†• ë„¤íŠ¸ì›Œí¬ ì‹œê°í™” íƒ­
            if enable_network_viz and "ğŸ†• ë„¤íŠ¸ì›Œí¬ ì‹œê°í™”" in tabs_dict:
                with tabs_dict["ğŸ†• ë„¤íŠ¸ì›Œí¬ ì‹œê°í™”"]:
                    st.header("ğŸ•¸ï¸ ì¸í„°ë™í‹°ë¸Œ ë„¤íŠ¸ì›Œí¬ ì‹œê°í™”")
                    st.markdown("ë„¤íŠ¸ì›Œí¬ êµ¬ì¡°ë¥¼ ì¸í„°ë™í‹°ë¸Œí•˜ê²Œ íƒìƒ‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë…¸ë“œë¥¼ í´ë¦­í•˜ì—¬ ìƒì„¸ ì •ë³´ë¥¼ í™•ì¸í•˜ê³ , í™•ëŒ€/ì¶•ì†Œ ë° ë“œë˜ê·¸ë¥¼ í†µí•´ ë„¤íŠ¸ì›Œí¬ë¥¼ íƒìƒ‰í•˜ì„¸ìš”.")
                    
                    # ë¶„ì„ ìœ í˜• ì„ íƒ
                    available_analyses = []
                    if analyze_ipc:
                        available_analyses.extend(["4ìë¦¬ IPC ì½”ë“œ", "8ìë¦¬ IPC ì½”ë“œ"])
                    if analyze_inventor:
                        available_analyses.append("ë°œëª…ì")
                    if analyze_applicant:
                        available_analyses.append("ì¶œì›ì¸")
                    
                    if available_analyses:
                        selected_analysis = st.selectbox("ë¶„ì„ ìœ í˜• ì„ íƒ", available_analyses)
                        
                        # í•´ë‹¹ ë¶„ì„ ê²°ê³¼ ê°€ì ¸ì˜¤ê¸°
                        if selected_analysis == "4ìë¦¬ IPC ì½”ë“œ":
                            current_results = results.get('ipc_4', {})
                        elif selected_analysis == "8ìë¦¬ IPC ì½”ë“œ":
                            current_results = results.get('ipc_8', {})
                        elif selected_analysis == "ë°œëª…ì":
                            current_results = results.get('inventor', {})
                        elif selected_analysis == "ì¶œì›ì¸":
                            current_results = results.get('applicant', {})
                        else:
                            current_results = {}
                        
                        if current_results:
                            # ê¸°ê°„ ì„ íƒ
                            available_periods = list(current_results.keys())
                            selected_period = st.selectbox("ê¸°ê°„ ì„ íƒ", available_periods)
                            
                            if selected_period in current_results:
                                edges_df = current_results[selected_period]['edges']
                                nodes_df = current_results[selected_period]['nodes']
                                
                                # ë„¤íŠ¸ì›Œí¬ ì‹œê°í™” ì„¤ì •
                                col1, col2, col3 = st.columns(3)
                                
                                with col1:
                                    viz_max_nodes = st.slider("í‘œì‹œí•  ìµœëŒ€ ë…¸ë“œ ìˆ˜", 20, 200, max_nodes, key="viz_max_nodes")
                                
                                with col2:
                                    viz_min_edge_weight = st.slider("ìµœì†Œ ì—£ì§€ ê°€ì¤‘ì¹˜", 1, 20, min_edge_weight, key="viz_min_edge")
                                
                                with col3:
                                    show_statistics = st.checkbox("ë„¤íŠ¸ì›Œí¬ í†µê³„ í‘œì‹œ", value=True)
                                
                                # ì¸í„°ë™í‹°ë¸Œ ë„¤íŠ¸ì›Œí¬ ê·¸ë˜í”„ ìƒì„±
                                if len(edges_df) > 0 and len(nodes_df) > 0:
                                    fig = create_interactive_network_graph(
                                        edges_df, nodes_df, 
                                        title=f"{selected_analysis} - {selected_period} ë„¤íŠ¸ì›Œí¬",
                                        node_size_metric=node_size_metric,
                                        node_color_metric=node_color_metric,
                                        max_nodes=viz_max_nodes,
                                        min_edge_weight=viz_min_edge_weight
                                    )
                                    
                                    if fig:
                                        st.plotly_chart(fig, use_container_width=True)
                                    
                                    # ë„¤íŠ¸ì›Œí¬ í†µê³„ í‘œì‹œ
                                    if show_statistics:
                                        st.subheader("ğŸ“Š ë„¤íŠ¸ì›Œí¬ í†µê³„")
                                        
                                        # ê¸°ë³¸ í†µê³„
                                        col1, col2, col3, col4 = st.columns(4)
                                        with col1:
                                            st.metric("ì´ ë…¸ë“œ ìˆ˜", len(nodes_df))
                                        with col2:
                                            st.metric("ì´ ì—£ì§€ ìˆ˜", len(edges_df))
                                        with col3:
                                            density = len(edges_df) / (len(nodes_df) * (len(nodes_df) - 1) / 2) if len(nodes_df) > 1 else 0
                                            st.metric("ë„¤íŠ¸ì›Œí¬ ë°€ë„", f"{density:.4f}")
                                        with col4:
                                            avg_degree = nodes_df['Degree'].mean() if 'Degree' in nodes_df.columns else 0
                                            st.metric("í‰ê·  ì—°ê²°ë„", f"{avg_degree:.2f}")
                                        
                                        # ì¤‘ì‹¬ì„± ë¶„í¬ ê·¸ë˜í”„
                                        if 'EC' in nodes_df.columns:
                                            fig_centrality = create_centrality_distribution_plot(nodes_df, f"{selected_analysis} - {selected_period} ì¤‘ì‹¬ì„± ë¶„í¬")
                                            if fig_centrality:
                                                st.plotly_chart(fig_centrality, use_container_width=True)
                                        
                                        # ì—°ê²°ë„ ë¶„í¬ ê·¸ë˜í”„
                                        if 'Degree' in nodes_df.columns:
                                            fig_degree = create_degree_distribution_plot(nodes_df, f"{selected_analysis} - {selected_period} ì—°ê²°ë„ ë¶„í¬")
                                            if fig_degree:
                                                st.plotly_chart(fig_degree, use_container_width=True)
                                        
                                        # ë„¤íŠ¸ì›Œí¬ ì¸ì‚¬ì´íŠ¸
                                        insights = extract_network_insights(edges_df, nodes_df, selected_analysis)
                                        
                                        if insights:
                                            st.subheader("ğŸ” ì£¼ìš” ì¸ì‚¬ì´íŠ¸")
                                            
                                            if 'top_ec' in insights:
                                                st.write(f"**ê°€ì¥ ì˜í–¥ë ¥ ìˆëŠ” ë…¸ë“œ (EC):** {insights['top_ec']['label']} (ì ìˆ˜: {insights['top_ec']['value']:.4f})")
                                            
                                            if 'top_bc' in insights:
                                                st.write(f"**ê°€ì¥ ì¤‘ìš”í•œ ì¤‘ê°œì (BC):** {insights['top_bc']['label']} (ì ìˆ˜: {insights['top_bc']['value']:.4f})")
                                            
                                            if 'strongest_connection' in insights:
                                                st.write(f"**ê°€ì¥ ê°•í•œ ì—°ê²°:** {insights['strongest_connection']['source']} â†” {insights['strongest_connection']['target']} (ê°€ì¤‘ì¹˜: {insights['strongest_connection']['weight']})")
                                
                                else:
                                    st.info("ì‹œê°í™”í•  ë„¤íŠ¸ì›Œí¬ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
                        
                        # êµ¬ê°„ë³„ ë„¤íŠ¸ì›Œí¬ ë¹„êµ (êµ¬ê°„ì´ ì—¬ëŸ¬ ê°œì¸ ê²½ìš°)
                        if len(current_results) > 1:
                            st.subheader("ğŸ”„ êµ¬ê°„ë³„ ë„¤íŠ¸ì›Œí¬ ë¹„êµ")
                            
                            # ë„¤íŠ¸ì›Œí¬ í†µê³„ ëŒ€ì‹œë³´ë“œ
                            stats_fig, stats_df = create_network_statistics_dashboard(current_results, selected_analysis)
                            if stats_fig:
                                st.plotly_chart(stats_fig, use_container_width=True)
                                
                                with st.expander("ë„¤íŠ¸ì›Œí¬ í†µê³„ ë°ì´í„° ë³´ê¸°"):
                                    st.dataframe(stats_df)
                            
                            # êµ¬ê°„ë³„ ë„¤íŠ¸ì›Œí¬ ë¹„êµ ì‹œê°í™”
                            comparison_fig = create_network_comparison_graph(
                                current_results, selected_analysis,
                                node_size_metric=node_size_metric,
                                node_color_metric=node_color_metric,
                                max_nodes=50
                            )
                            
                            if comparison_fig:
                                st.plotly_chart(comparison_fig, use_container_width=True)
                    else:
                        st.info("ë¶„ì„ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € ë¶„ì„ì„ ì‹¤í–‰í•´ì£¼ì„¸ìš”.")
            
            # ì‹œê³„ì—´ ëŒ€ì‹œë³´ë“œ íƒ­
            if enable_timeseries:
                with tabs_dict["ì‹œê³„ì—´ ëŒ€ì‹œë³´ë“œ"]:
                    st.header("ì‹œê³„ì—´ ë¶„ì„ ëŒ€ì‹œë³´ë“œ")
                    
                    # ê° ë¶„ì„ í•­ëª©ë³„ ì‹œê³„ì—´ í†µê³„ ìƒì„± ë° ì‹œê°í™”
                    if analyze_ipc and 'ì¶œì›ì¼' in df.columns:
                        st.subheader("IPC ì½”ë“œ ì‹œê³„ì—´ ë¶„ì„")
                        ipc_stats = calculate_timeseries_stats(df, 'ì¶œì›ì¼', selected_columns['ipc'], "IPC")
                        if not ipc_stats.empty:
                            periods_info = [{'start': info['start'], 'end': info['end']} for info in periods.values()] if periods else None
                            fig_ipc = create_timeseries_visualization(ipc_stats, "IPC", periods_info)
                            if fig_ipc:
                                st.plotly_chart(fig_ipc, use_container_width=True)
                    
                    if analyze_inventor and 'ì¶œì›ì¼' in df.columns:
                        st.subheader("ë°œëª…ì ì‹œê³„ì—´ ë¶„ì„")
                        inventor_stats = calculate_timeseries_stats(df, 'ì¶œì›ì¼', selected_columns['inventor'], "ë°œëª…ì")
                        if not inventor_stats.empty:
                            periods_info = [{'start': info['start'], 'end': info['end']} for info in periods.values()] if periods else None
                            fig_inventor = create_timeseries_visualization(inventor_stats, "ë°œëª…ì", periods_info)
                            if fig_inventor:
                                st.plotly_chart(fig_inventor, use_container_width=True)
                    
                    if analyze_applicant and 'ì¶œì›ì¼' in df.columns:
                        st.subheader("ì¶œì›ì¸ ì‹œê³„ì—´ ë¶„ì„")
                        applicant_stats = calculate_timeseries_stats(df, 'ì¶œì›ì¼', selected_columns['applicant'], "ì¶œì›ì¸")
                        if not applicant_stats.empty:
                            periods_info = [{'start': info['start'], 'end': info['end']} for info in periods.values()] if periods else None
                            fig_applicant = create_timeseries_visualization(applicant_stats, "ì¶œì›ì¸", periods_info)
                            if fig_applicant:
                                st.plotly_chart(fig_applicant, use_container_width=True)
            
            # ì¤‘ì‹¬ì„± ì¶”ì´ ë¶„ì„ íƒ­
            if enable_timeseries and "ì¤‘ì‹¬ì„± ì¶”ì´ ë¶„ì„" in tabs_dict:
                with tabs_dict["ì¤‘ì‹¬ì„± ì¶”ì´ ë¶„ì„"]:
                    st.header("ğŸ“ˆ ì¤‘ì‹¬ì„± ì§€í‘œ ì—°ë„ë³„ ì¶”ì´ ë¶„ì„")
                    st.markdown("ì—°ë„ë³„ ì¤‘ì‹¬ì„± ì§€í‘œ ë³€í™”ë¥¼ ì¶”ì í•˜ì—¬ í•µì‹¬ ë…¸ë“œë“¤ì˜ ì˜í–¥ë ¥ ë³€í™”ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤.")
                    
                    # ìƒìœ„ Nê°œ ë…¸ë“œ ì„ íƒ ìŠ¬ë¼ì´ë”
                    if "top_n_nodes" not in st.session_state:
                        st.session_state.top_n_nodes = 10

                    top_n = st.slider("í‘œì‹œí•  ìƒìœ„ ë…¸ë“œ ìˆ˜", min_value=3, max_value=20, value=st.session_state.top_n_nodes, step=1)
                    st.session_state.top_n_nodes = top_n
                    
                    if analyze_ipc and 'ipc_4' in yearly_centrality_results:
                        st.subheader("ğŸ”¬ 4ìë¦¬ IPC ì½”ë“œ ì¤‘ì‹¬ì„± ì¶”ì´")
                        if yearly_centrality_results['ipc_4']:
                            fig_ipc4 = create_centrality_trend_visualization(yearly_centrality_results['ipc_4'], "4ìë¦¬ IPC ì½”ë“œ", top_n)
                            if fig_ipc4:
                                st.plotly_chart(fig_ipc4, use_container_width=True)
                        else:
                            st.info("4ìë¦¬ IPC ì½”ë“œ ì—°ë„ë³„ ì¤‘ì‹¬ì„± ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")

                    if analyze_ipc and 'ipc_8' in yearly_centrality_results:
                        st.subheader("ğŸ”¬ 8ìë¦¬ IPC ì½”ë“œ ì¤‘ì‹¬ì„± ì¶”ì´")
                        if yearly_centrality_results['ipc_8']:
                            fig_ipc8 = create_centrality_trend_visualization(yearly_centrality_results['ipc_8'], "8ìë¦¬ IPC ì½”ë“œ", top_n)
                            if fig_ipc8:
                                st.plotly_chart(fig_ipc8, use_container_width=True)
                        else:
                            st.info("8ìë¦¬ IPC ì½”ë“œ ì—°ë„ë³„ ì¤‘ì‹¬ì„± ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")

                    # ì¤‘ì‹¬ì„± ì¶”ì´ ë¶„ì„ - IPC ì¤‘ì‹¬ì„± ì¶”ì„¸ ë…¸ë“œ ì¶œë ¥
                    for ipc_level in ['ipc_4', 'ipc_8']:
                        if ipc_level in yearly_centrality_results:
                            entity_label = "4ìë¦¬ IPC ì½”ë“œ" if ipc_level == 'ipc_4' else "8ìë¦¬ IPC ì½”ë“œ"
                            st.subheader(f"ğŸ“Š {entity_label} ì¶”ì„¸ ë¶„ì„")

                            result = yearly_centrality_results[ipc_level]

                            for metric in ['EC', 'BC', 'CC']:
                                st.markdown(f"### `{metric}` ê¸°ì¤€ ì¶”ì„¸ ë¶„ì„")
                                inc_nodes, dec_nodes = identify_monotonic_centrality_nodes(result, metric)

                                col1, col2 = st.columns(2)

                                with col1:
                                    st.markdown(f"**ğŸ”¼ ê¾¸ì¤€íˆ ì¦ê°€í•œ ë…¸ë“œ (ìƒìœ„ 10ê°œ)**")
                                    if inc_nodes:
                                        for node, values in inc_nodes[:10]:
                                            trend_str = " â†’ ".join(f"{v:.4f}" for v in values)
                                            st.write(f"â€¢ {node}: {trend_str}")
                                    else:
                                        st.write("ì—†ìŒ")

                                with col2:
                                    st.markdown(f"**ğŸ”½ ê¾¸ì¤€íˆ ê°ì†Œí•œ ë…¸ë“œ (ìƒìœ„ 10ê°œ)**")
                                    if dec_nodes:
                                        for node, values in dec_nodes[:10]:
                                            trend_str = " â†’ ".join(f"{v:.4f}" for v in values)
                                            st.write(f"â€¢ {node}: {trend_str}")
                                    else:
                                        st.write("ì—†ìŒ")
                    
                    if analyze_inventor and 'inventor' in yearly_centrality_results:
                        st.subheader("ğŸ‘¨â€ğŸ’¼ ë°œëª…ì ì¤‘ì‹¬ì„± ì¶”ì´")
                        if yearly_centrality_results['inventor']:
                            fig_inventor = create_centrality_trend_visualization(yearly_centrality_results['inventor'], "ë°œëª…ì", top_n)
                            if fig_inventor:
                                st.plotly_chart(fig_inventor, use_container_width=True)
                        else:
                            st.info("ë°œëª…ì ì—°ë„ë³„ ì¤‘ì‹¬ì„± ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
                    
                    if analyze_applicant and 'applicant' in yearly_centrality_results:
                        st.subheader("ğŸ¢ ì¶œì›ì¸ ì¤‘ì‹¬ì„± ì¶”ì´")
                        if yearly_centrality_results['applicant']:
                            fig_applicant = create_centrality_trend_visualization(yearly_centrality_results['applicant'], "ì¶œì›ì¸", top_n)
                            if fig_applicant:
                                st.plotly_chart(fig_applicant, use_container_width=True)
                        else:
                            st.info("ì¶œì›ì¸ ì—°ë„ë³„ ì¤‘ì‹¬ì„± ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            
            # êµ¬ê°„ë³„ ë³€í™” ë¶„ì„ íƒ­
            if enable_timeseries and "êµ¬ê°„ë³„ ë³€í™” ë¶„ì„" in tabs_dict:
                with tabs_dict["êµ¬ê°„ë³„ ë³€í™” ë¶„ì„"]:
                    st.header("ğŸ”„ êµ¬ê°„ë³„ ë³€í™” ë¶„ì„")
                    st.markdown("ì„¤ì •í•œ êµ¬ê°„ë“¤ ê°„ì˜ ë…¸ë“œ ë“±ì¥/ì†Œë©¸ ë° ì¤‘ì‹¬ì„± ì§€í‘œ ë³€í™”ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤.")
                    
                    if len(periods) >= 1:
                        # IPC ì½”ë“œ êµ¬ê°„ë³„ ë³€í™” ë¶„ì„
                        if analyze_ipc:
                            # 4ìë¦¬ IPC ì½”ë“œ
                            st.subheader("ğŸ”¬ 4ìë¦¬ IPC ì½”ë“œ êµ¬ê°„ë³„ ë³€í™”")
                            changes_analysis_ipc4, centrality_changes_ipc4 = analyze_period_changes(results['ipc_4'], "4ìë¦¬ IPC ì½”ë“œ")
                            display_period_changes_analysis(changes_analysis_ipc4, centrality_changes_ipc4, "4ìë¦¬ IPC ì½”ë“œ")
                            
                            # 8ìë¦¬ IPC ì½”ë“œ
                            st.subheader("ğŸ”¬ 8ìë¦¬ IPC ì½”ë“œ êµ¬ê°„ë³„ ë³€í™”")
                            changes_analysis_ipc8, centrality_changes_ipc8 = analyze_period_changes(results['ipc_8'], "8ìë¦¬ IPC ì½”ë“œ")
                            display_period_changes_analysis(changes_analysis_ipc8, centrality_changes_ipc8, "8ìë¦¬ IPC ì½”ë“œ")
                        
                        # ë°œëª…ì êµ¬ê°„ë³„ ë³€í™” ë¶„ì„
                        if analyze_inventor:
                            st.subheader("ğŸ‘¨â€ğŸ’¼ ë°œëª…ì êµ¬ê°„ë³„ ë³€í™”")
                            changes_analysis_inv, centrality_changes_inv = analyze_period_changes(results['inventor'], "ë°œëª…ì")
                            display_period_changes_analysis(changes_analysis_inv, centrality_changes_inv, "ë°œëª…ì")
                        
                        # ì¶œì›ì¸ êµ¬ê°„ë³„ ë³€í™” ë¶„ì„
                        if analyze_applicant:
                            st.subheader("ğŸ¢ ì¶œì›ì¸ êµ¬ê°„ë³„ ë³€í™”")
                            changes_analysis_app, centrality_changes_app = analyze_period_changes(results['applicant'], "ì¶œì›ì¸")
                            display_period_changes_analysis(changes_analysis_app, centrality_changes_app, "ì¶œì›ì¸")
                    else:
                        st.info("êµ¬ê°„ë³„ ë³€í™” ë¶„ì„ì„ ìœ„í•´ì„œëŠ” ìµœì†Œ 1ê°œ ì´ìƒì˜ êµ¬ê°„ì„ ì„¤ì •í•´ì£¼ì„¸ìš”.")
            
            # ê° ë¶„ì„ ê²°ê³¼ íƒ­ì— ìƒì„¸ ì •ë³´ í‘œì‹œ
            if analyze_ipc:
                # 4ìë¦¬ IPC ì½”ë“œ ë¶„ì„ ê²°ê³¼ íƒ­
                with tabs_dict["4ìë¦¬ IPC ì½”ë“œ"]:
                    st.header("4ìë¦¬ IPC ì½”ë“œ ë¶„ì„ ê²°ê³¼")
                    
                    # êµ¬ê°„ë³„ ê²°ê³¼ í‘œì‹œ
                    for period_name, result in results['ipc_4'].items():
                        st.subheader(f"{period_name} ë¶„ì„ ê²°ê³¼")
                        
                        nodes_df = result['nodes']
                        edges_df = result['edges']
                        
                        if len(nodes_df) > 0:
                            # ë„¤íŠ¸ì›Œí¬ ì‹œê°í™” (ê°„ë‹¨í•œ ë²„ì „)
                            if enable_network_viz and len(edges_df) > 0:
                                with st.expander(f"{period_name} ë„¤íŠ¸ì›Œí¬ ì‹œê°í™” ë³´ê¸°"):
                                    fig_simple = create_interactive_network_graph(
                                        edges_df, nodes_df, 
                                        title=f"4ìë¦¬ IPC ì½”ë“œ - {period_name}",
                                        max_nodes=50, min_edge_weight=1
                                    )
                                    if fig_simple:
                                        st.plotly_chart(fig_simple, use_container_width=True)
                            
                            # ì¤‘ì‹¬ì„± ì§€í‘œ ìƒìœ„ ë…¸ë“œë“¤ í‘œì‹œ
                            if calculate_centrality and 'EC' in nodes_df.columns:
                                col1, col2, col3 = st.columns(3)
                                
                                with col1:
                                    display_top_centrality_nodes(nodes_df, 'EC', '4ìë¦¬ IPC ì½”ë“œ', 5)
                                with col2:
                                    display_top_centrality_nodes(nodes_df, 'BC', '4ìë¦¬ IPC ì½”ë“œ', 5)
                                with col3:
                                    display_top_centrality_nodes(nodes_df, 'CC', '4ìë¦¬ IPC ì½”ë“œ', 5)
                            
                            # ìƒìœ„ ë™ì‹œì¶œí˜„ ìŒ í‘œì‹œ
                            display_top_pairs(edges_df, nodes_df, "IPC ìŒ")
                            
                            # ë…¸ë“œ ë°ì´í„° í‘œì‹œ (ìƒìœ„ 50ê°œ)
                            with st.expander(f"{period_name} ë…¸ë“œ ë°ì´í„° ë³´ê¸°"):
                                st.dataframe(nodes_df.head(50))
                            
                            # ì—£ì§€ ë°ì´í„° í‘œì‹œ (ìƒìœ„ 20ê°œ)
                            with st.expander(f"{period_name} ì—£ì§€ ë°ì´í„° ë³´ê¸°"):
                                st.dataframe(edges_df.sort_values(by='Weight', ascending=False).head(20))
                        else:
                            st.warning(f"{period_name}ì— ë¶„ì„í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
                    
                    # êµ¬ê°„ë³„ ë¹„êµ ë¶„ì„
                    if len(results['ipc_4']) > 1:
                        display_period_comparison(results['ipc_4'], "4ìë¦¬ IPC ì½”ë“œ")
                
                # 8ìë¦¬ IPC ì½”ë“œ ë¶„ì„ ê²°ê³¼ íƒ­
                with tabs_dict["8ìë¦¬ IPC ì½”ë“œ"]:
                    st.header("8ìë¦¬ IPC ì½”ë“œ ë¶„ì„ ê²°ê³¼")
                    
                    # êµ¬ê°„ë³„ ê²°ê³¼ í‘œì‹œ
                    for period_name, result in results['ipc_8'].items():
                        st.subheader(f"{period_name} ë¶„ì„ ê²°ê³¼")
                        
                        nodes_df = result['nodes']
                        edges_df = result['edges']
                        
                        if len(nodes_df) > 0:
                            # ë„¤íŠ¸ì›Œí¬ ì‹œê°í™” (ê°„ë‹¨í•œ ë²„ì „)
                            if enable_network_viz and len(edges_df) > 0:
                                with st.expander(f"{period_name} ë„¤íŠ¸ì›Œí¬ ì‹œê°í™” ë³´ê¸°"):
                                    fig_simple = create_interactive_network_graph(
                                        edges_df, nodes_df, 
                                        title=f"8ìë¦¬ IPC ì½”ë“œ - {period_name}",
                                        max_nodes=50, min_edge_weight=1
                                    )
                                    if fig_simple:
                                        st.plotly_chart(fig_simple, use_container_width=True)
                            
                            # ì¤‘ì‹¬ì„± ì§€í‘œ ìƒìœ„ ë…¸ë“œë“¤ í‘œì‹œ
                            if calculate_centrality and 'EC' in nodes_df.columns:
                                col1, col2, col3 = st.columns(3)
                                
                                with col1:
                                    display_top_centrality_nodes(nodes_df, 'EC', '8ìë¦¬ IPC ì½”ë“œ', 5)
                                with col2:
                                    display_top_centrality_nodes(nodes_df, 'BC', '8ìë¦¬ IPC ì½”ë“œ', 5)
                                with col3:
                                    display_top_centrality_nodes(nodes_df, 'CC', '8ìë¦¬ IPC ì½”ë“œ', 5)
                            
                            # ìƒìœ„ ë™ì‹œì¶œí˜„ ìŒ í‘œì‹œ
                            display_top_pairs(edges_df, nodes_df, "IPC ìŒ")
                            
                            # ë…¸ë“œ ë°ì´í„° í‘œì‹œ (ìƒìœ„ 50ê°œ)
                            with st.expander(f"{period_name} ë…¸ë“œ ë°ì´í„° ë³´ê¸°"):
                                st.dataframe(nodes_df.head(50))
                            
                            # ì—£ì§€ ë°ì´í„° í‘œì‹œ (ìƒìœ„ 20ê°œ)
                            with st.expander(f"{period_name} ì—£ì§€ ë°ì´í„° ë³´ê¸°"):
                                st.dataframe(edges_df.sort_values(by='Weight', ascending=False).head(20))
                        else:
                            st.warning(f"{period_name}ì— ë¶„ì„í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
                    
                    # êµ¬ê°„ë³„ ë¹„êµ ë¶„ì„
                    if len(results['ipc_8']) > 1:
                        display_period_comparison(results['ipc_8'], "8ìë¦¬ IPC ì½”ë“œ")
            
            if analyze_inventor:
                # ë°œëª…ì ë¶„ì„ ê²°ê³¼ íƒ­
                with tabs_dict["ë°œëª…ì ë¶„ì„"]:
                    st.header("ë°œëª…ì ë¶„ì„ ê²°ê³¼")
                    
                    # êµ¬ê°„ë³„ ê²°ê³¼ í‘œì‹œ
                    for period_name, result in results['inventor'].items():
                        st.subheader(f"{period_name} ë¶„ì„ ê²°ê³¼")
                        
                        nodes_df = result['nodes']
                        edges_df = result['edges']
                        
                        if len(nodes_df) > 0:
                            # ë„¤íŠ¸ì›Œí¬ ì‹œê°í™” (ê°„ë‹¨í•œ ë²„ì „)
                            if enable_network_viz and len(edges_df) > 0:
                                with st.expander(f"{period_name} ë„¤íŠ¸ì›Œí¬ ì‹œê°í™” ë³´ê¸°"):
                                    fig_simple = create_interactive_network_graph(
                                        edges_df, nodes_df, 
                                        title=f"ë°œëª…ì - {period_name}",
                                        max_nodes=50, min_edge_weight=1
                                    )
                                    if fig_simple:
                                        st.plotly_chart(fig_simple, use_container_width=True)
                            
                            # ì¤‘ì‹¬ì„± ì§€í‘œ ìƒìœ„ ë…¸ë“œë“¤ í‘œì‹œ
                            if calculate_centrality and 'EC' in nodes_df.columns:
                                col1, col2, col3 = st.columns(3)
                                
                                with col1:
                                    display_top_centrality_nodes(nodes_df, 'EC', 'ë°œëª…ì', 5)
                                with col2:
                                    display_top_centrality_nodes(nodes_df, 'BC', 'ë°œëª…ì', 5)
                                with col3:
                                    display_top_centrality_nodes(nodes_df, 'CC', 'ë°œëª…ì', 5)
                            
                            # ìƒìœ„ ë™ì‹œì¶œí˜„ ìŒ í‘œì‹œ
                            display_top_pairs(edges_df, nodes_df, "ë°œëª…ì ìŒ")
                            
                            # ë…¸ë“œ ë°ì´í„° í‘œì‹œ (ìƒìœ„ 50ê°œ)
                            with st.expander(f"{period_name} ë…¸ë“œ ë°ì´í„° ë³´ê¸°"):
                                st.dataframe(nodes_df.head(50))
                            
                            # ì—£ì§€ ë°ì´í„° í‘œì‹œ (ìƒìœ„ 20ê°œ)
                            with st.expander(f"{period_name} ì—£ì§€ ë°ì´í„° ë³´ê¸°"):
                                st.dataframe(edges_df.sort_values(by='Weight', ascending=False).head(20))
                        else:
                            st.warning(f"{period_name}ì— ë¶„ì„í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
                    
                    # êµ¬ê°„ë³„ ë¹„êµ ë¶„ì„
                    if len(results['inventor']) > 1:
                        display_period_comparison(results['inventor'], "ë°œëª…ì")
            
            if analyze_applicant:
                # ì¶œì›ì¸ ë¶„ì„ ê²°ê³¼ íƒ­
                with tabs_dict["ì¶œì›ì¸ ë¶„ì„"]:
                    st.header("ì¶œì›ì¸ ë¶„ì„ ê²°ê³¼")
                    
                    # êµ¬ê°„ë³„ ê²°ê³¼ í‘œì‹œ
                    for period_name, result in results['applicant'].items():
                        st.subheader(f"{period_name} ë¶„ì„ ê²°ê³¼")
                        
                        nodes_df = result['nodes']
                        edges_df = result['edges']
                        
                        if len(nodes_df) > 0:
                            # ë„¤íŠ¸ì›Œí¬ ì‹œê°í™” (ê°„ë‹¨í•œ ë²„ì „)
                            if enable_network_viz and len(edges_df) > 0:
                                with st.expander(f"{period_name} ë„¤íŠ¸ì›Œí¬ ì‹œê°í™” ë³´ê¸°"):
                                    fig_simple = create_interactive_network_graph(
                                        edges_df, nodes_df, 
                                        title=f"ì¶œì›ì¸ - {period_name}",
                                        max_nodes=50, min_edge_weight=1
                                    )
                                    if fig_simple:
                                        st.plotly_chart(fig_simple, use_container_width=True)
                            
                            # ì¤‘ì‹¬ì„± ì§€í‘œ ìƒìœ„ ë…¸ë“œë“¤ í‘œì‹œ
                            if calculate_centrality and 'EC' in nodes_df.columns:
                                col1, col2, col3 = st.columns(3)
                                
                                with col1:
                                    display_top_centrality_nodes(nodes_df, 'EC', 'ì¶œì›ì¸', 5)
                                with col2:
                                    display_top_centrality_nodes(nodes_df, 'BC', 'ì¶œì›ì¸', 5)
                                with col3:
                                    display_top_centrality_nodes(nodes_df, 'CC', 'ì¶œì›ì¸', 5)
                            
                            # ìƒìœ„ ë™ì‹œì¶œí˜„ ìŒ í‘œì‹œ
                            display_top_pairs(edges_df, nodes_df, "ì¶œì›ì¸ ìŒ")
                            
                            # ë…¸ë“œ ë°ì´í„° í‘œì‹œ (ìƒìœ„ 50ê°œ)
                            with st.expander(f"{period_name} ë…¸ë“œ ë°ì´í„° ë³´ê¸°"):
                                st.dataframe(nodes_df.head(50))
                            
                            # ì—£ì§€ ë°ì´í„° í‘œì‹œ (ìƒìœ„ 20ê°œ)
                            with st.expander(f"{period_name} ì—£ì§€ ë°ì´í„° ë³´ê¸°"):
                                st.dataframe(edges_df.sort_values(by='Weight', ascending=False).head(20))
                        else:
                            st.warning(f"{period_name}ì— ë¶„ì„í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
                    
                    # êµ¬ê°„ë³„ ë¹„êµ ë¶„ì„
                    if len(results['applicant']) > 1:
                        display_period_comparison(results['applicant'], "ì¶œì›ì¸")
    else:
        with tabs_dict["ë¶„ì„ ê²°ê³¼"]:
            st.info("ğŸ‘ˆ ì‚¬ì´ë“œë°”ì—ì„œ Excel íŠ¹í—ˆ ë°ì´í„° íŒŒì¼ì„ ì—…ë¡œë“œí•˜ê³  ë¶„ì„ ì˜µì…˜ì„ ì„ íƒí•œ í›„ 'ë¶„ì„ ì‹œì‘' ë²„íŠ¼ì„ í´ë¦­í•˜ì„¸ìš”.")
            st.info("í•„ìš”í•œ ê²½ìš° IPC ì½”ë“œ ë ˆì´ë¸” ë§¤í•‘ íŒŒì¼ë„ í•¨ê»˜ ì—…ë¡œë“œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
            
            st.markdown("""
            ### ğŸ“‹ ë°ì´í„° í˜•ì‹ ìš”êµ¬ì‚¬í•­
            
            **ì¶œì›ì¼ ì»¬ëŸ¼**: ë‹¤ì–‘í•œ ë‚ ì§œ í˜•ì‹ ì§€ì›
            - ì˜ˆì‹œ: `2023-01-15`, `2023/01/15`, `2023.01.15`, `20230115`, `2023-01`, `2023`
            
            **ë°œëª…ì ì»¬ëŸ¼**: ì—¬ëŸ¬ ë°œëª…ìëŠ” `|` êµ¬ë¶„ìë¡œ ë¶„ë¦¬
            - ì˜ˆì‹œ: `í™ê¸¸ë™|ê¹€ì² ìˆ˜|ì´ì˜í¬`
            
            **ì¶œì›ì¸ ì»¬ëŸ¼**: ì—¬ëŸ¬ ì¶œì›ì¸ì€ `|` êµ¬ë¶„ìë¡œ ë¶„ë¦¬  
            - ì˜ˆì‹œ: `ì‚¼ì„±ì „ì|LGì „ì`
            
            **IPC ì½”ë“œ**: ê¸°ì¡´ í˜•ì‹ ìœ ì§€
            - ì˜ˆì‹œ: `B66B-001/34[H01L-021/00,G06F-015/16]`
            
            ### ğŸ“Š ì¤‘ì‹¬ì„± ì§€í‘œ ì„¤ëª…
            
            **EC (Eigenvector Centrality)**: ê³ ìœ ë²¡í„° ì¤‘ì‹¬ì„±
            - ì¤‘ìš”í•œ ë…¸ë“œë“¤ê³¼ ì—°ê²°ëœ ë…¸ë“œì˜ ì¤‘ìš”ë„ë¥¼ ì¸¡ì •
            - ê°’ì´ ë†’ì„ìˆ˜ë¡ ì˜í–¥ë ¥ ìˆëŠ” ë…¸ë“œë“¤ê³¼ ì—°ê²°ë˜ì–´ ìˆìŒ
            
            **BC (Betweenness Centrality)**: ë§¤ê°œ ì¤‘ì‹¬ì„±
            - ë‹¤ë¥¸ ë…¸ë“œë“¤ ì‚¬ì´ì˜ ìµœë‹¨ ê²½ë¡œì— ìœ„ì¹˜í•˜ëŠ” ì •ë„
            - ê°’ì´ ë†’ì„ìˆ˜ë¡ ë„¤íŠ¸ì›Œí¬ì—ì„œ ì¤‘ê°œ ì—­í• ì´ í¼
            
            **CC (Closeness Centrality)**: ê·¼ì ‘ ì¤‘ì‹¬ì„±
            - ë‹¤ë¥¸ ëª¨ë“  ë…¸ë“œë“¤ê³¼ì˜ í‰ê·  ê±°ë¦¬ì˜ ì—­ìˆ˜
            - ê°’ì´ ë†’ì„ìˆ˜ë¡ ë‹¤ë¥¸ ëª¨ë“  ë…¸ë“œë“¤ì— ë¹ ë¥´ê²Œ ì ‘ê·¼ ê°€ëŠ¥
            
            ### â° ì‹œê³„ì—´ ë¶„ì„ ê¸°ëŠ¥
            
            **êµ¬ê°„ë³„ ë¹„êµ ë¶„ì„**: ìµœëŒ€ 3ê°œ êµ¬ê°„ ì„¤ì • ê°€ëŠ¥
            - ê° êµ¬ê°„ë³„ë¡œ ë„¤íŠ¸ì›Œí¬ êµ¬ì¡° ë³€í™” ì¶”ì 
            - ì‹œê¸°ë³„ ì¤‘ì‹¬ì„± ì§€í‘œ ë³€í™” ë¶„ì„
            
            **ì‹œê³„ì—´ ëŒ€ì‹œë³´ë“œ**: ì—°ë„ë³„ í†µê³„ ì‹œê°í™”
            - íŠ¹í—ˆ ì¶œì› íŠ¸ë Œë“œ ë¶„ì„
            - ê³ ìœ  ì—”í‹°í‹° ìˆ˜ ë³€í™” ì¶”ì 
            - êµ¬ê°„ë³„ í•˜ì´ë¼ì´íŠ¸ í‘œì‹œ
            
            ### ğŸ†• ìƒˆë¡œìš´ ë„¤íŠ¸ì›Œí¬ ì‹œê°í™” ê¸°ëŠ¥
            
            **ì¸í„°ë™í‹°ë¸Œ ë„¤íŠ¸ì›Œí¬ ê·¸ë˜í”„**:
            - ë…¸ë“œì™€ ì—£ì§€ë¥¼ í´ë¦­í•˜ì—¬ ìƒì„¸ ì •ë³´ í™•ì¸
            - í™•ëŒ€/ì¶•ì†Œ ë° ë“œë˜ê·¸ë¥¼ í†µí•œ ììœ ë¡œìš´ íƒìƒ‰
            - ì¤‘ì‹¬ì„± ì§€í‘œì— ë”°ë¥¸ ë…¸ë“œ í¬ê¸°/ìƒ‰ìƒ ì¡°ì •
            
            **ë„¤íŠ¸ì›Œí¬ í†µê³„ ëŒ€ì‹œë³´ë“œ**:
            - êµ¬ê°„ë³„ ë„¤íŠ¸ì›Œí¬ ë°€ë„, í´ëŸ¬ìŠ¤í„°ë§ ê³„ìˆ˜ ë“± ë¹„êµ
            - ì¤‘ì‹¬ì„± ì§€í‘œ ë¶„í¬ íˆìŠ¤í† ê·¸ë¨
            - ì—°ê²°ë„ ë¶„í¬ ë° ë„¤íŠ¸ì›Œí¬ ì¸ì‚¬ì´íŠ¸
            
            **ì»¤ë®¤ë‹ˆí‹° íƒì§€**:
            - ìë™ ì»¤ë®¤ë‹ˆí‹° íƒì§€ ë° ì‹œê°í™”
            - ì»¤ë®¤ë‹ˆí‹°ë³„ ìƒ‰ìƒ êµ¬ë¶„
            - ë„¤íŠ¸ì›Œí¬ êµ¬ì¡° íŒ¨í„´ ë¶„ì„
            
            **êµ¬ê°„ë³„ ë„¤íŠ¸ì›Œí¬ ë¹„êµ**:
            - ì—¬ëŸ¬ ì‹œê¸°ì˜ ë„¤íŠ¸ì›Œí¬ë¥¼ í•œ ë²ˆì— ë¹„êµ
            - ì‹œê³„ì—´ ë„¤íŠ¸ì›Œí¬ êµ¬ì¡° ë³€í™” ì¶”ì 
            - ë„¤íŠ¸ì›Œí¬ ì§„í™” íŒ¨í„´ ë¶„ì„
            
            **ğŸ›ï¸ ê³ ê¸‰ ì„¤ì • ê¸°ëŠ¥**:
            - ğŸ“ˆ **ì¤‘ì‹¬ì„± ì¶”ì´ ë¶„ì„**: ì—°ë„ë³„ ì¤‘ì‹¬ì„± ì§€í‘œ ë³€í™” ì‹œê°í™”
            - ğŸ”„ **êµ¬ê°„ë³„ ë³€í™” ë¶„ì„**: ì‹ ê·œ/ì†Œë©¸ ë…¸ë“œ ë° ì¤‘ì‹¬ì„± ë³€í™” ì¶”ì 
            - ğŸšï¸ **ì‹œê°í™” ì˜µì…˜ ì¡°ì ˆ**: ë…¸ë“œ ìˆ˜, ì—£ì§€ ê°€ì¤‘ì¹˜, ìƒ‰ìƒ/í¬ê¸° ê¸°ì¤€ ì„¤ì •
            - ğŸ“Š **ì‹¤ì‹œê°„ ë„¤íŠ¸ì›Œí¬ í†µê³„**: ë°€ë„, í´ëŸ¬ìŠ¤í„°ë§, ì¤‘ì‹¬ì„± ë¶„í¬ ë“±
            
            **ì¶œë ¥ íŒŒì¼**: êµ¬ê°„ë³„ ë…ë¦½ íŒŒì¼ ìƒì„±
            - ê° êµ¬ê°„ì— ëŒ€í•œ ë³„ë„ì˜ ë…¸ë“œ/ì—£ì§€ íŒŒì¼
            - Gephiì—ì„œ ì‹œê³„ì—´ ë¹„êµ ë¶„ì„ ê°€ëŠ¥
            """)
        
        # ì‹œê³„ì—´ ëŒ€ì‹œë³´ë“œ íƒ­ (íŒŒì¼ ì—…ë¡œë“œ ì „ì—ë„ ì„¤ëª… í‘œì‹œ)
        with tabs_dict["ì‹œê³„ì—´ ëŒ€ì‹œë³´ë“œ"]:
            st.header("ì‹œê³„ì—´ ë¶„ì„ ëŒ€ì‹œë³´ë“œ")
            if uploaded_file is None:
                st.info("ë°ì´í„°ë¥¼ ì—…ë¡œë“œí•˜ê³  ë¶„ì„ì„ ì‹œì‘í•˜ë©´ ì‹œê³„ì—´ ì°¨íŠ¸ê°€ ì—¬ê¸°ì— í‘œì‹œë©ë‹ˆë‹¤.")
                
                st.markdown("""
                ### ì‹œê³„ì—´ ëŒ€ì‹œë³´ë“œ ê¸°ëŠ¥
                
                1. **ì—°ë„ë³„ íŠ¹í—ˆ ê±´ìˆ˜**: ì‹œê°„ì— ë”°ë¥¸ íŠ¹í—ˆ ì¶œì› ì¶”ì„¸
                2. **ê³ ìœ  ì—”í‹°í‹° ìˆ˜**: ê° ì—°ë„ë³„ ê³ ìœ í•œ IPC/ë°œëª…ì/ì¶œì›ì¸ ìˆ˜
                3. **í‰ê·  ì—”í‹°í‹° ìˆ˜**: íŠ¹í—ˆë‹¹ í‰ê·  IPC/ë°œëª…ì/ì¶œì›ì¸ ìˆ˜
                4. **ëˆ„ì  íŠ¹í—ˆ ê±´ìˆ˜**: ì‹œê°„ì— ë”°ë¥¸ ëˆ„ì  íŠ¹í—ˆ ìˆ˜
                5. **êµ¬ê°„ í•˜ì´ë¼ì´íŠ¸**: ì„¤ì •í•œ ë¹„êµ êµ¬ê°„ì„ ì°¨íŠ¸ì— í‘œì‹œ
                """)
        
        # ë„¤íŠ¸ì›Œí¬ ì‹œê°í™” íƒ­ (íŒŒì¼ ì—…ë¡œë“œ ì „ ì„¤ëª…)
        if enable_network_viz and "ğŸ†• ë„¤íŠ¸ì›Œí¬ ì‹œê°í™”" in tabs_dict:
            with tabs_dict["ğŸ†• ë„¤íŠ¸ì›Œí¬ ì‹œê°í™”"]:
                st.header("ğŸ•¸ï¸ ì¸í„°ë™í‹°ë¸Œ ë„¤íŠ¸ì›Œí¬ ì‹œê°í™”")
                if uploaded_file is None:
                    st.info("ë°ì´í„°ë¥¼ ì—…ë¡œë“œí•˜ê³  ë¶„ì„ì„ ì‹œì‘í•˜ë©´ ì¸í„°ë™í‹°ë¸Œ ë„¤íŠ¸ì›Œí¬ ê·¸ë˜í”„ê°€ ì—¬ê¸°ì— í‘œì‹œë©ë‹ˆë‹¤.")
                    
                    st.markdown("""
                    ### ğŸ†• ë„¤íŠ¸ì›Œí¬ ì‹œê°í™” ê¸°ëŠ¥ ì†Œê°œ
                    
                    **ì¸í„°ë™í‹°ë¸Œ ë„¤íŠ¸ì›Œí¬ ê·¸ë˜í”„**:
                    - ğŸ–±ï¸ **ë§ˆìš°ìŠ¤ ì¸í„°ë™ì…˜**: ë…¸ë“œ í´ë¦­ìœ¼ë¡œ ìƒì„¸ ì •ë³´ í™•ì¸
                    - ğŸ” **í™•ëŒ€/ì¶•ì†Œ**: ë§ˆìš°ìŠ¤ íœ ë¡œ ë„¤íŠ¸ì›Œí¬ ì„¸ë¶€ íƒìƒ‰
                    - ğŸ¨ **ë™ì  ìƒ‰ìƒ/í¬ê¸°**: ì¤‘ì‹¬ì„± ì§€í‘œì— ë”°ë¥¸ ë…¸ë“œ ì‹œê°í™”
                    - ğŸ“ **í˜¸ë²„ ì •ë³´**: ë…¸ë“œì™€ ì—£ì§€ì˜ ìƒì„¸ ë°ì´í„° ì‹¤ì‹œê°„ í‘œì‹œ
                    
                    **ê³ ê¸‰ í•„í„°ë§ ì˜µì…˜**:
                    - ğŸ“Š **ë…¸ë“œ ìˆ˜ ì œí•œ**: ì„±ëŠ¥ ìµœì í™”ë¥¼ ìœ„í•œ ìƒìœ„ Nê°œ ë…¸ë“œ ì„ íƒ
                    - âš–ï¸ **ì—£ì§€ ê°€ì¤‘ì¹˜ í•„í„°**: ì•½í•œ ì—°ê²° ì œê±°ë¡œ í•µì‹¬ êµ¬ì¡° ê°•ì¡°
                    - ğŸ¯ **ì¤‘ì‹¬ì„± ê¸°ë°˜ ì •ë ¬**: EC, BC, CC ë“± ë‹¤ì–‘í•œ ì¤‘ì‹¬ì„± ì§€í‘œ í™œìš©
                    
                    **ë„¤íŠ¸ì›Œí¬ í†µê³„ ëŒ€ì‹œë³´ë“œ**:
                    - ğŸ“ˆ **ì‹¤ì‹œê°„ í†µê³„**: ë…¸ë“œ ìˆ˜, ì—£ì§€ ìˆ˜, ë°€ë„, í‰ê·  ì—°ê²°ë„
                    - ğŸ“Š **ë¶„í¬ íˆìŠ¤í† ê·¸ë¨**: ì¤‘ì‹¬ì„± ì§€í‘œ ë° ì—°ê²°ë„ ë¶„í¬ ì‹œê°í™”
                    - ğŸ” **ë„¤íŠ¸ì›Œí¬ ì¸ì‚¬ì´íŠ¸**: í•µì‹¬ ë…¸ë“œ ë° ê°•í•œ ì—°ê²° ìë™ ì‹ë³„
                    
                    **ì»¤ë®¤ë‹ˆí‹° íƒì§€**:
                    - ğŸ¨ **ìë™ ìƒ‰ìƒ ë¶„ë¥˜**: ì»¤ë®¤ë‹ˆí‹°ë³„ ë…¸ë“œ ìƒ‰ìƒ êµ¬ë¶„
                    - ğŸ”— **ëª¨ë“ˆì„± ë¶„ì„**: ë„¤íŠ¸ì›Œí¬ ë‚´ ê·¸ë£¹ êµ¬ì¡° íŒŒì•…
                    - ğŸ“‹ **ì»¤ë®¤ë‹ˆí‹° ì •ë³´**: ê° ë…¸ë“œì˜ ì†Œì† ì»¤ë®¤ë‹ˆí‹° í‘œì‹œ
                    
                    **êµ¬ê°„ë³„ ë¹„êµ ì‹œê°í™”**:
                    - ğŸ“… **ì‹œê³„ì—´ ë„¤íŠ¸ì›Œí¬**: ì—¬ëŸ¬ ì‹œê¸°ì˜ ë„¤íŠ¸ì›Œí¬ ë™ì‹œ ë¹„êµ
                    - ğŸ”„ **êµ¬ì¡° ë³€í™” ì¶”ì **: ì‹œê°„ì— ë”°ë¥¸ ë„¤íŠ¸ì›Œí¬ ì§„í™” íŒ¨í„´
                    - ğŸ“Š **í†µê³„ ëŒ€ì‹œë³´ë“œ**: êµ¬ê°„ë³„ ë„¤íŠ¸ì›Œí¬ ì§€í‘œ ë¹„êµ
                    """)

# ì•± ì‹¤í–‰
if __name__ == "__main__":
    main()

# ì¶”ê°€ ë„ìš°ë¯¸ í•¨ìˆ˜ë“¤
def create_network_summary_report(results_dict, analysis_type):
    """
    ë„¤íŠ¸ì›Œí¬ ë¶„ì„ ìš”ì•½ ë³´ê³ ì„œë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    """
    if not results_dict:
        return None
    
    summary = f"# {analysis_type} ë„¤íŠ¸ì›Œí¬ ë¶„ì„ ìš”ì•½ ë³´ê³ ì„œ\n\n"
    
    for period_name, result in results_dict.items():
        edges_df = result['edges']
        nodes_df = result['nodes']
        
        if len(nodes_df) == 0:
            continue
        
        summary += f"## {period_name}\n\n"
        
        # ê¸°ë³¸ í†µê³„
        summary += f"- **ë…¸ë“œ ìˆ˜**: {len(nodes_df)}ê°œ\n"
        summary += f"- **ì—£ì§€ ìˆ˜**: {len(edges_df)}ê°œ\n"
        
        if len(edges_df) > 0:
            # ë„¤íŠ¸ì›Œí¬ ë°€ë„
            max_edges = len(nodes_df) * (len(nodes_df) - 1) / 2
            density = len(edges_df) / max_edges if max_edges > 0 else 0
            summary += f"- **ë„¤íŠ¸ì›Œí¬ ë°€ë„**: {density:.4f}\n"
            
            # ê°€ì¥ ê°•í•œ ì—°ê²°
            strongest_edge = edges_df.loc[edges_df['Weight'].idxmax()]
            source_name = nodes_df.loc[nodes_df['id'] == strongest_edge['Source'], 'Name'].iloc[0]
            target_name = nodes_df.loc[nodes_df['id'] == strongest_edge['Target'], 'Name'].iloc[0]
            summary += f"- **ê°€ì¥ ê°•í•œ ì—°ê²°**: {source_name} â†” {target_name} (ê°€ì¤‘ì¹˜: {strongest_edge['Weight']})\n"
        
        # ì¤‘ì‹¬ì„± ì§€í‘œ ìƒìœ„ ë…¸ë“œ
        if 'EC' in nodes_df.columns:
            top_ec = nodes_df.loc[nodes_df['EC'].idxmax()]
            summary += f"- **ìµœê³  ê³ ìœ ë²¡í„° ì¤‘ì‹¬ì„±**: {top_ec['Name']} ({top_ec['EC']:.4f})\n"
        
        if 'BC' in nodes_df.columns:
            top_bc = nodes_df.loc[nodes_df['BC'].idxmax()]
            summary += f"- **ìµœê³  ë§¤ê°œ ì¤‘ì‹¬ì„±**: {top_bc['Name']} ({top_bc['BC']:.4f})\n"
        
        if 'CC' in nodes_df.columns:
            top_cc = nodes_df.loc[nodes_df['CC'].idxmax()]
            summary += f"- **ìµœê³  ê·¼ì ‘ ì¤‘ì‹¬ì„±**: {top_cc['Name']} ({top_cc['CC']:.4f})\n"
        
        summary += "\n---\n\n"
    
    return summary

def export_network_to_graphml(edges_df, nodes_df, filename):
    """
    ë„¤íŠ¸ì›Œí¬ ë°ì´í„°ë¥¼ GraphML í˜•ì‹ìœ¼ë¡œ ë‚´ë³´ëƒ…ë‹ˆë‹¤.
    """
    try:
        G = nx.Graph()
        
        # ë…¸ë“œ ì¶”ê°€
        for _, node in nodes_df.iterrows():
            G.add_node(node['id'], **node.to_dict())
        
        # ì—£ì§€ ì¶”ê°€
        for _, edge in edges_df.iterrows():
            G.add_edge(edge['Source'], edge['Target'], weight=edge['Weight'])
        
        # GraphMLë¡œ ì €ì¥
        nx.write_graphml(G, filename)
        return True
    except Exception as e:
        st.error(f"GraphML ë‚´ë³´ë‚´ê¸° ì˜¤ë¥˜: {e}")
        return False

def calculate_network_evolution_metrics(period_results):
    """
    ë„¤íŠ¸ì›Œí¬ ì§„í™” ì§€í‘œë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.
    """
    if len(period_results) < 2:
        return None
    
    period_names = list(period_results.keys())
    evolution_metrics = {}
    
    for i in range(1, len(period_names)):
        prev_period = period_names[i-1]
        curr_period = period_names[i]
        
        prev_nodes = set(period_results[prev_period]['nodes']['Name'].tolist())
        curr_nodes = set(period_results[curr_period]['nodes']['Name'].tolist())
        
        # ë…¸ë“œ ì§„í™” ì§€í‘œ
        stability = len(prev_nodes & curr_nodes) / len(prev_nodes | curr_nodes) if len(prev_nodes | curr_nodes) > 0 else 0
        growth_rate = (len(curr_nodes) - len(prev_nodes)) / len(prev_nodes) if len(prev_nodes) > 0 else 0
        turnover_rate = len(prev_nodes ^ curr_nodes) / len(prev_nodes | curr_nodes) if len(prev_nodes | curr_nodes) > 0 else 0
        
        evolution_metrics[f"{prev_period} â†’ {curr_period}"] = {
            'stability': stability,
            'growth_rate': growth_rate,
            'turnover_rate': turnover_rate,
            'new_nodes': len(curr_nodes - prev_nodes),
            'lost_nodes': len(prev_nodes - curr_nodes),
            'total_nodes_prev': len(prev_nodes),
            'total_nodes_curr': len(curr_nodes)
        }
    
    return evolution_metrics

def create_evolution_visualization(evolution_metrics):
    """
    ë„¤íŠ¸ì›Œí¬ ì§„í™” ì§€í‘œë¥¼ ì‹œê°í™”í•©ë‹ˆë‹¤.
    """
    if not evolution_metrics:
        return None
    
    transitions = list(evolution_metrics.keys())
    stability_scores = [evolution_metrics[t]['stability'] for t in transitions]
    growth_rates = [evolution_metrics[t]['growth_rate'] for t in transitions]
    turnover_rates = [evolution_metrics[t]['turnover_rate'] for t in transitions]
    
    fig = make_subplots(
        rows=1, cols=3,
        subplot_titles=['ë„¤íŠ¸ì›Œí¬ ì•ˆì •ì„±', 'ì„±ì¥ë¥ ', 'êµì²´ìœ¨'],
        specs=[[{"secondary_y": False}, {"secondary_y": False}, {"secondary_y": False}]]
    )
    
    # ì•ˆì •ì„±
    fig.add_trace(
        go.Bar(x=transitions, y=stability_scores, name='ì•ˆì •ì„±', marker_color='blue'),
        row=1, col=1
    )
    
    # ì„±ì¥ë¥ 
    fig.add_trace(
        go.Bar(x=transitions, y=growth_rates, name='ì„±ì¥ë¥ ', 
               marker_color=['green' if x >= 0 else 'red' for x in growth_rates]),
        row=1, col=2
    )
    
    # êµì²´ìœ¨
    fig.add_trace(
        go.Bar(x=transitions, y=turnover_rates, name='êµì²´ìœ¨', marker_color='orange'),
        row=1, col=3
    )
    
    fig.update_layout(
        title='ë„¤íŠ¸ì›Œí¬ ì§„í™” ì§€í‘œ',
        showlegend=False,
        height=400
    )
    
    return fig

# ë§ˆì§€ë§‰ìœ¼ë¡œ ì¶”ê°€ì ì¸ ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜
def validate_network_data(edges_df, nodes_df):
    """
    ë„¤íŠ¸ì›Œí¬ ë°ì´í„°ì˜ ìœ íš¨ì„±ì„ ê²€ì‚¬í•©ë‹ˆë‹¤.
    """
    issues = []
    
    if len(nodes_df) == 0:
        issues.append("ë…¸ë“œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
    
    if len(edges_df) == 0:
        issues.append("ì—£ì§€ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
    
    # ë…¸ë“œ ID ì¤‘ë³µ ê²€ì‚¬
    if len(nodes_df) > 0 and nodes_df['id'].duplicated().any():
        issues.append("ì¤‘ë³µëœ ë…¸ë“œ IDê°€ ìˆìŠµë‹ˆë‹¤.")
    
    # ì—£ì§€ ì°¸ì¡° ìœ íš¨ì„± ê²€ì‚¬
    if len(edges_df) > 0 and len(nodes_df) > 0:
        valid_node_ids = set(nodes_df['id'])
        invalid_sources = ~edges_df['Source'].isin(valid_node_ids)
        invalid_targets = ~edges_df['Target'].isin(valid_node_ids)
        
        if invalid_sources.any() or invalid_targets.any():
            issues.append("ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ë…¸ë“œë¥¼ ì°¸ì¡°í•˜ëŠ” ì—£ì§€ê°€ ìˆìŠµë‹ˆë‹¤.")
    
    return issues

def optimize_network_layout(G, layout_algorithm='spring'):
    """
    ë„¤íŠ¸ì›Œí¬ ë ˆì´ì•„ì›ƒì„ ìµœì í™”í•©ë‹ˆë‹¤.
    """
    layouts = {
        'spring': nx.spring_layout,
        'circular': nx.circular_layout,
        'random': nx.random_layout,
        'shell': nx.shell_layout,
        'spectral': nx.spectral_layout
    }
    
    if layout_algorithm in layouts:
        try:
            return layouts[layout_algorithm](G, seed=42)
        except:
            # Fallback to spring layout
            return nx.spring_layout(G, seed=42)
    else:
        return nx.spring_layout(G, seed=42)

# ì„±ëŠ¥ ìµœì í™”ë¥¼ ìœ„í•œ ìºì‹± í•¨ìˆ˜ë“¤
@st.cache_data
def cached_centrality_calculation(edges_list, nodes_list):
    """
    ì¤‘ì‹¬ì„± ê³„ì‚°ì„ ìºì‹±í•©ë‹ˆë‹¤.
    """
    edges_df = pd.DataFrame(edges_list)
    nodes_df = pd.DataFrame(nodes_list)
    return calculate_centrality_measures(edges_df, nodes_df)

@st.cache_data  
def cached_layout_calculation(edges_list, nodes_list, algorithm='spring'):
    """
    ë ˆì´ì•„ì›ƒ ê³„ì‚°ì„ ìºì‹±í•©ë‹ˆë‹¤.
    """
    G = nx.Graph()
    
    for node in nodes_list:
        G.add_node(node['id'])
    
    for edge in edges_list:
        G.add_edge(edge['Source'], edge['Target'], weight=edge['Weight'])
    
    return optimize_network_layout(G, algorithm)
